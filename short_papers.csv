number,forum,title,abstract,authors,decision
1,https://openreview.net/forum?id=uEuoKy2hUkm,Echocardiographic Phase Detection Using Neural Networks,"Accurate identification of end-diastolic and end-systolic frames in echocardiographic cine loops is essential when measuring cardiac function. Manual selection by human experts is challenging and error prone. This paper presents a deep neural network trained and tested on multi-centre patient data for accurate phase detection in apical four-chamber videos of arbitrary length, spanning several heartbeats, with performance indistinguishable from that of human experts.",Elisabeth Sarah Lane|Neda Azarmehr|Jevgeni Jevsikov|James P Howard|Matthew Shun-shin|Darrel P Francis|Massoud Zolgharni,Accept (Poster)
4,https://openreview.net/forum?id=h3HC1EU7AEz,ViT-V-Net: Vision Transformer for Unsupervised Volumetric Medical Image Registration,"In the last decade, convolutional neural networks (ConvNets) have dominated and achieved state-of-the-art performances in a variety of medical imaging applications. However, the performances of ConvNets are still limited by lacking the understanding of long-range spatial relations in an image. The recently proposed Vision Transformer (ViT) for image classification uses a purely self-attention-based model that learns long-range spatial relations to focus on the relevant parts of an image. Nevertheless, ViT emphasizes the low-resolution features because of the consecutive downsamplings, result in a lack of detailed localization information, making it unsuitable for image registration. Recently, several ViT-based image segmentation methods have been combined with ConvNets to improve the recovery of detailed localization information. Inspired by them, we present ViT-V-Net, which bridges ViT and ConvNet to provide volumetric medical image registration. The experimental results presented here demonstrate that the proposed architecture achieves superior performance to several top-performing registration methods.",Junyu Chen|Yufan He|Eric Frey|Ye Li|Yong Du,Accept (Poster)
5,https://openreview.net/forum?id=pOFGaVQeXAk,Deep Clustering Activation Maps for Emphysema Subtyping,"We propose a deep learning clustering method that exploits dense features from a segmentation network for emphysema subtyping from computed tomography (CT) scans. Using dense features enables high-resolution visualization of image regions corresponding to the cluster assignment via dense clustering activation maps (dCAMs). This approach provides model interpretability. We evaluated clustering results on 500 subjects from the COPDGene study, where radiologists manually annotated emphysema sub-types according to their visual CT assessment. We achieved a 43% unsupervised clustering accuracy, outperforming our baseline at 41% and yielding results comparable to supervised classification at 45%. The proposed method also offers a better cluster formation than the baseline, achieving 0.54 in silhouette coefficient and 0.55 in David-Bouldin scores.",Weiyi Xie|Colin Jacobs|Bram van Ginneken,Accept (Poster)
6,https://openreview.net/forum?id=9CSM4yQmZiN,Learning to predict cutting angles from histological human brain sections,"Studying brain architecture at the cellular level requires histological image analysis of sectioned postmortem samples. We trained a deep neural network to estimate relative angles between the cutting plane and the local 3D brain surface from 2D cortical image patches sampled from microscopic scans of human brain tissue sections. The model allows to automatically identify obliquely cut tissue parts, which often confuse downstream texture classification tasks and typically require specific treatment in image analysis workflows. It has immediate applications for the automated analysis of brain structures, like cytoarchitectonic mapping of the highly convoluted human brain.
",Christian Schiffer|Luisa Schuhmacher|Katrin Amunts|Timo Dickscheid,Accept (Poster)
8,https://openreview.net/forum?id=fnb58KJtYv,50 shades of overfitting:  towards MRI-based neurologicalmodels interpretation,"MRI-based prediction models are one of the most exploited AI solutions in neurology. Numerous computer-vision models showed their predictive ability for diverse psychoneurological conditions. Although most of these models are based on weak or no annotation, only a few reported studies interpret the predictions and perform the model saliency regions' analysis.

We utilize 3DCNN interpretation with GradCAM to explore learned patterns for basic demographic characteristics prediction on the healthy cohort. We compare the saliency maps of the gender prediction models with the different types of MRI data preprocessing and augmentation. We assess the quality of learned patterns and examine the ways of models overfitting. We propose a data augmentation strategy based on optimal transport to avoid model overfitting on the brain volumes.",Polina Druzhinina|Ekaterina Kondrateva|Evgeny Burnaev,Accept (Poster)
13,https://openreview.net/forum?id=OcATYbGIxv4,Quantifying the Scanner-Induced Domain Gap in Mitosis Detection,"Automated detection of mitotic figures in histopathology images has seen vast improvements, thanks to modern deep learning-based pipelines. Application of these methods, however, is in practice limited by strong variability of images between labs. This results in a domain shift of the images, which causes a performance drop of the models. Hypothesizing that the scanner device plays a decisive role in this effect, we evaluated the susceptibility of a standard mitosis detection approach to the domain shift introduced by using a different whole slide scanner. Our work is based on the MICCAI-MIDOG challenge 2021 data set, which includes 200 tumor cases of human breast cancer and four scanners.  

Our work indicates that the domain shift induced not by biochemical variability but purely by the choice of acquisition device is underestimated so far. Models trained on images of the same scanner yielded an average F1 score of 0.683, while models trained on a single other scanner only yielded an average F1 score of 0.325. Training on another multi-domain mitosis dataset led to mean F1 scores of 0.52. We found this not to be reflected by domain-shifts measured as proxy A distance-derived metric.",Marc Aubreville,Accept (Poster)
15,https://openreview.net/forum?id=1TPRpNyyj2L,Carbon footprint driven deep learning model selection for medical imaging,"Selecting task appropriate deep learning models is a resource intensive process; more so when working with large quantities of high dimensional data that are encountered in medical imaging. Model selection procedures that are primarily aimed at improving performance measures such as accuracy could become biased towards resource intensive models. In this work, we propose to inform and drive the model selection procedure using the carbon footprint of training deep learning models as a complementary measure along with other standard performance metrics. We experimentally demonstrate that increasing carbon footprint of large models might not necessarily translate into proportional performance gains, and suggest useful trade-offs to obtain resource efficient models.",Raghavendra Selvan,Accept (Poster)
19,https://openreview.net/forum?id=RFwhfWEMyzm,Exploiting Adam-like Optimization Algorithms to Improve the Performance of Convolutional Neural Networks,"Stochastic gradient descent (SGD) is the main approach for training deep networks: it moves towards the optimum of the cost function by  iteratively updating the parameters of a model in the direction of the gradient of the loss evaluated on a minibatch. Several variants of SGD have been proposed to make adaptive step sizes for each parameter (adaptive gradient) and take into account the previous updates (momentum). Among several alternative of SGD the most popular are AdaGrad, AdaDelta, RMSProp and Adam which scale coordinates of the gradient by square roots of some form of averaging of the squared coordinates in the past gradients and automatically adjust the learning rate on a parameter basis. In this work, we compare Adam based variants based on the difference between the present and the past gradients, the step size is adjusted for each parameter. We run several tests benchmarking proposed methods using medical image data. The experiments are performed using ResNet50 architecture neural network. Moreover, we have tested ensemble of networks and the fusion with ResNet50 trained with stochastic gradient descent. To combine the set of ResNet50 the simple sum rule has been applied. Proposed ensemble obtains very high performance, it obtains accuracy comparable or better than actual state of the art. To improve reproducibility and research efficiency the MATLAB source code used for this research is available at GitHub: https://github.com/LorisNanni.",Loris Nanni|Gianluca Maguolo|Alessandra Lumini,Accept (Poster)
20,https://openreview.net/forum?id=164F3ixC9Jc,ICAM-reg: Interpretable Classification and Regression with Feature Attribution for Mapping Neurological Phenotypes in Individual Scans,"Feature attribution (FA), or the assignment of class-relevance to different locations in an image, is important for many classification and regression problems but is particularly crucial within the neuroscience domain, where accurate mechanistic models of behaviours, or disease, require knowledge of all features discriminative of a trait. At the same time, predicting class relevance from brain images is challenging as phenotypes are typically heterogeneous, and changes occur against a background of significant natural variation.  Here, we present an extension of the ICAM framework for creating prediction specific FA maps through image-to-image translation.",Cher Bass|Mariana da Silva|Carole H. Sudre|Logan Zane John Williams|Petru-Daniel Tudosiu|Fidel Alfaro-Almagro|Sean P. Fitzgibbon|Matthew Glasser|Stephen M. Smith|Emma Claire Robinson,Accept (Poster)
21,https://openreview.net/forum?id=76X9Mthzv4X,Common limitations of performance metrics in biomedical image analysis,"While the importance of automatic biomedical image analysis is increasing at an enormous pace, recent meta-research revealed major flaws with respect to algorithm validation. Performance metrics are key for objective, transparent and comparative performance assessment, but little attention has been given to their pitfalls. Under the umbrella of the Helmholtz Imaging Platform (HIP), three international initiatives - the MICCAI Society's challenge working group, the Biomedical Image Analysis Challenges (BIAS) initiative, as well as the benchmarking working group of the MONAI framework - have now joined forces with the mission to generate best practice recommendations with respect to metrics in medical image analysis. Consensus building is achieved via a Delphi process, a popular tool for integrating opinions in large international consortia. The current document serves as a teaser for the results presentation and focuses on the pitfalls of the most commonly used metric in biomedical image analysis, the Dice Similarity Coefficient (DSC), in the categories of (1) mathematical properties/edge cases, (2) task/metric fit and (3) metric aggregation. Being compiled by a large group of experts from more than 30 institutes worldwide, we believe that our framework could be of general interest to the MIDL community and will improve the quality of biomedical image analysis algorithm validation. ",Annika Reinke|Matthias Eisenmann|Minu Dietlinde Tizabi|Carole H. Sudre|Tim Rädsch|Michela Antonelli|Tal Arbel|Spyridon Bakas|M. Jorge Cardoso|Veronika Cheplygina|Keyvan Farahani|Ben Glocker|Doreen Heckmann-Nötzel|Fabian Isensee|Pierre Jannin|Charles Kahn|Jens Kleesiek|Tahsin Kurc|Michal Kozubek|Bennett A. Landman|Geert Litjens|Klaus Maier-Hein|Anne Lousise Martel|bjoern menze|Henning Müller|Jens Petersen|Mauricio Reyes|Nicola Rieke|Bram Stieltjes|Ronald M. Summers|Sotirios A. Tsaftaris|Bram van Ginneken|Annette Kopp-Schneider|Paul Jäger|Lena Maier-Hein,Accept (Poster)
24,https://openreview.net/forum?id=NJcszyl19PN,Deep ensembles based on Stochastic Activation Selection for Polyp Segmentation,"Semantic segmentation has a wide array of applications ranging from medical-image analysis, scene understanding, autonomous driving and robotic navigation. This work deals with medical image segmentation and in particular with accurate polyp detection and segmentation during colonoscopy examinations. Several convolutional neural network architectures have been proposed to effectively deal with this task and with the problem of segmenting objects at different scale input. The basic architecture in image segmentation consists of an encoder and a decoder: the first uses convolutional filters to extract features from the image, the second is responsible for generating the final output. In this work, we compare some variant of the DeepLab architecture obtained by varying the decoder backbone. We compare several decoder architectures, including ResNet, Xception, EfficentNet, MobileNet and we perturb their layers by substituting ReLU activation layers with other functions. The resulting methods are used to create deep ensembles which are shown to be very effective. Our experimental evaluations show that our best ensemble produces good segmentation results by achieving high evaluation scores with a dice coefficient of  0.884, and a mean Intersection over Union (mIoU) of 0.818 for the Kvasir-SEG dataset. To improve reproducibility and research efficiency the MATLAB source code used for this research is available at GitHub: https://github.com/LorisNanni.",Alessandra Lumini|Loris Nanni|Gianluca Maguolo,Accept (Poster)
25,https://openreview.net/forum?id=XCRthDLsCXn,Partial Convolution Network for Metal Artifact Reduction in CT Preprocessing: Preliminary Results,"Metal artifacts impair the diagnostic value of medical CT images. These artifacts occur from the projection values associated with the metal objects inside the scanned anatomy. In this work, we replace the corrupted projection values by using a deep convolutional neural network consisting of so-called partial convolution layers. 
We show that the network trained on simulated data enhances newly presented projection data and therefore the corresponding reconstructed image. ",Laura Hellwege|Nele Blum|Thorsten Buzug|Maik Stille,Accept (Poster)
28,https://openreview.net/forum?id=84NU3uAj1HW,Projection Domain Metal Artifact Reduction in Computed Tomography using Conditional Generative Adversarial Networks,"High-density objects in the field of view, still remain one of the major challenges in CT
image reconstruction. They cause artifacts in the image, which degrade the quality and
the diagnostic value of the image. Standard approaches for metal artifact reduction are
often unable to correct these artifacts sufficiently or introduce new artifacts. In this work,
a new deep learning approach for the reduction of metal artifacts in CT images is proposed
using a Generative Adversarial Network. A generator network is applied directly to the
projection data corrupted by the metal objects to learn the corrected data. In addition, a
second network, the discriminator, is used to evaluate the quality of the learned data. The
results of the trained generator network show that most of the data could be reasonably
replaced by the network, reducing the artifacts in the reconstructed image.",Nele Blum|Thorsten Buzug|Maik Stille,Accept (Poster)
31,https://openreview.net/forum?id=eehADvdlUa3,Morphology-based losses for weakly supervised segmentation of mammograms,"Segmentation is one of the most common tasks in medical imaging, but it often requires expensive ground truth for training. Weakly supervised methods cope with the lack of annotations, however, they often fall short compared to fully supervised ones. In this work, we propose to constrain the segmentation output with morphological operations, leading to an increase in the overall performance. In particular, we use top-hat and closing operations. We evaluate the method on high-resolution images from INBreast dataset and achieve an increase in F$_1$ of $\approx 0.14$ and in recall of $\approx 0.22$ compared to the training without morphology loss.",Mickael Tardy|Diana Mateus,Accept (Poster)
33,https://openreview.net/forum?id=GHNGMR1EAtN,Predicting molecular subtypes of breast cancer using multimodal deep learning and incorporation of the attention mechanism,"Accurately determining the molecular subtype of breast cancer is an important factor for the prognosis of breast cancer patients, and can guide treatment selection. In this study, we report a multimodal deep learning with attention mechanism (MDLA) for predicting the molecular subtypes of breast cancer from mammography and ultrasound images. Incorporation of the attention mechanism improved diagnostic performance for predicting 4-class molecular subtypes with Matthews correlation coefficient (MCC) of 0.794. The MDLA can also discriminate between Luminal disease and non-luminal disease with areas under the receiver operating characteristic curve (AUC) of 0.855. This work thus provides a noninvasive imaging biomarker to predict the molecular subtypes of breast cancer.",Tianyu Zhang|Luyi Han|Yuan Gao|Xin Wang|Regina Beets-Tan|Ritse Mann,Accept (Poster)
34,https://openreview.net/forum?id=dVUHL5QhDhL,Efficient Video-Based Deep Learning for Ultrasound Guided Needle Insertion,"We investigate video-based deep learning approaches for detecting needle insertions in ultrasound videos. We introduce two efficient and conceptually simple extensions to convert standard 2D object detectors into video object detectors that make use of temporal information from a history of frames. We compare our approaches to a 2D baseline method that makes independent predictions per frame. Given the need to run in real-time on computationally restricted environments, emphasis is placed on low computational complexity.",Jonathan Rubin|Alvin Chen|Anumod Odungattu Thodiyil|Raghavendra Srinivasa Naidu|Ramon Erkamp|Jon Fincke|Balasundar Raju,Accept (Poster)
35,https://openreview.net/forum?id=rKiYUGvII6,mGEV: Extension of the GEV Activation to Multiclass Classification,"Unbalanced data poses a challenge when training machine learning algorithms; the algorithm often overfits on the dominant class and neglects the smaller classes. While methods such as oversampling aim to rebalance the data, this can lead to overfitting. When a certain class is underrepresented, either because it a rare disease or few images exist then methods are needed which can adequately account for this. The generalized extreme value (GEV) activation has recently been proposed as a solution to highly unbalanced data; however, the GEV activation is only available for binary classification. We extend this to the multiclass case with the multiclass GEV (mGEV) activation. We conduct experiments on X-ray images, with three classes, showing much-improved performance over the commonly used softmax activation. Code for the mGEV activation is available at [https://github.com/JTBridge/GEV].",Joshua Thomas Bridge|Yalin Zheng,Accept (Poster)
36,https://openreview.net/forum?id=oJi6xpSLdsj,VinDr-RibCXR: A Benchmark Dataset for Automatic Segmentation and Labeling of Individual Ribs on Chest X-rays,"Segmenting and labeling correctly the individual ribs from chest radiograph (CXR) are of significant clinical value for several diagnostic tasks. Developing automatic deep learning (DL) algorithms for this task requires annotated images of the ribs at pixel-level. However, to the best of our knowledge, there exists no such public datasets as well as benchmark protocols for performance evaluation. To solve this problem, we establish a new CXR dataset, namely VinDr-RibCXR, for automatically segmenting and labeling of individual ribs. The VinDr-RibCXR contains 245 posteroanterior CXRs with corresponding segmentation annotations for each rib provided by human experts. Furthermore, we train the state-of-the-art DL-based segmentation models on 196 images from the RibCXR and report performance of those models on an independent test set of 49 images. Our best performing DL model (i.e., Nested U-Net with EfficientNet-B0)  obtains  a  Dice  score  of  0.834 (95% CI, 0.810-0.853). The  sensitivity,  specificity  and  Hausdorff distance are 0.841 (95% CI, 0.812-0.858), 0.998 (95% CI, 0.997-0.998), and 15.453 (95% CI, 13.340-17.450), respectively. These results demonstrate a high-level of performance in labeling of the individual ribs from CXRs. Our study, therefore, serves as a proof of concept and baseline performance for future research. The dataset, codes, and trained DL models will be made publicly available to encourage new advances in this research direction.",Hoang Canh Nguyen|Tung Thanh Le|Hieu Pham|Ha Quy Nguyen,Accept (Poster)
37,https://openreview.net/forum?id=O9EWFKXcXTU,ProtoBrainMaps: Prototypical Brain Maps for Alzheimer's Disease Progression Modeling,"Discovering the brain progression over a lifetime is beneficial for identifying the subject affected by neurodegenerative disorders, such as Alzheimer's disease (AD) which require detection at the earliest possible time for the sake of delaying the progression by the virtue of particular treatments. As brain progressions in terms of both normal aging and AD-pathology tend to be entangled to each other, distinguishing the progression pathways of AD over the normal aging brains is quite an intricate task. To this end, we propose Prototypical Brain Maps (ProtoBrainMaps) for modeling the AD progressions through the established prototypes in the latent space via clinically-guided topological maps. Having devised as an interpretable network, it possesses the ability to establish and synthesize a set of well-interpolated prototypical brains, each corresponding to certain health conditions in terms of neurodegenerative diseases. ",Ahmad Wisnu Mulyadi|Heung-Il Suk,Accept (Poster)
46,https://openreview.net/forum?id=JG895xlWsfA,Multichannel input pixelwise regression 3D U-Nets for medical image estimation with 3 applications in brain MRI,"The U-Net is a robust general-purpose deep learning architecture designed for semantic segmentation of medical images, and has been extended to 3D for volumetric applications such as magnetic resonance imaging (MRI) of the human brain. An adaptation of the U-Net to output pixelwise regression values, instead of class labels, based on multichannel input data, has been developed in the remote sensing satellite imaging research domain. The pixelwise regression U-Net has only received limited consideration as a deep learning architecture in medical imaging for the image estimation/synthesis problem, and the limited work so far did not consider the application of 3D multichannel inputs. In this paper, we propose the use of the multichannel input pixelwise regression 3D U-Net (rUNet) for estimation of medical images. Our findings demonstrate that this approach is robust and versatile and can be applied to predicting a pending MRI examination of patients with Alzheimer's disease based on previous rounds of imaging, can perform medical image reconstruction (parametric mapping) in diffusion MRI, and can be applied to the estimation of one type of MRI examination from a collection of other types. Results demonstrate that the rUNet represents a single deep learning architecture capable of solving a variety of image estimation problems. Public domain code is provided.",Jueqi Wang|Derek Berger|David Mattie|Jacob Levman,Accept (Poster)
47,https://openreview.net/forum?id=70gFxx5ytwh,Double adversarial domain adaptation for whole-slide-imageclassification,"Image classification on whole-slide-image (WSI) is a challenging task. A previous work based on Fisher vector encoding provided a novel end-to-end pipeline with promising accuracy and computational efficiency.
However, this pipeline suffers from an accuracy drop when deployed to another dataset to perform the same task.
This poses a limitation on the practical use of the pipeline especially when the diagnoses of WSIs are hard to obtain.
This paper aims at providing a solution to mitigate the accuracy drop by using an unsupervised domain adaptation approach.
We propose to insert the domain classifiers into the pipeline in two stages to align the features during training. 
We evaluate accuracy by calculating the confusion matrices before and after the adaptation on two datasets. We demonstrate that placing domain classifiers in different stages will boost accuracy.
",Yuchen Yang|Amir Akbarnejad|Nilanjan Ray|Gilbert Bigras,Accept (Poster)
49,https://openreview.net/forum?id=pL_aFZKNO5N,Me-NDT: Neural-backed Decision Tree for Visual Explainability of Deep Medical Models,"Despite the progress of deep learning on medical imaging, there is still not a true understanding of what networks learn and of how decisions are reached. Here, we address this by proposing a Visualized Neural-backed Decision Tree for Medical image analysis, Me-NDT. It is a CNN with a tree-based structure template that allows for both classification and visualization of firing neurons, thus offering interpretability. We also introduce node and path losses that allow Me-NDT to consider the entire path instead of isolated nodes. Our experiments on brain CT and chest radiographs outperform all baselines. Overall, Me-NDT is a lighter, comprehensively explanatory model, of great value for clinical practice. ",Guanghui FU|Ruiqian Wang|Jianqiang Li|Maria Vakalopoulou|Vicky Kalogeiton,Accept (Poster)
50,https://openreview.net/forum?id=9o6zjvbo7b0,An artificial intelligence system for predicting the deterioration of COVID-19 patients in the emergency department,"During the COVID-19 pandemic, rapid and accurate triage of patients at the emergency department is critical to inform decision-making. We propose a data-driven approach for prediction of deterioration risk using a deep neural network that learns from chest X-ray images, and a gradient boosting model that learns from routine clinical variables. Our AI prognosis system, trained using data from 3,661 patients, achieves the AUC of 0.786 (95% CI: 0.742-0.827) when predicting deterioration within 96 hours. Our deep neural network indicates informative areas of chest X-ray images to assist clinicians in interpreting the predictions, and performs comparably to two experienced chest radiologists in a reader study. In summary, our findings demonstrate the potential of the proposed system for assisting front-line physicians in the triage of COVID-19 patients.",Farah Shamout|Yiqiu Shen|Nan Wu|Aakash Kaku|Jungkyu Park|Taro Makino|Stanisław Jastrzębski|Jan Witowski|Duo Wang|Ben Zhang|Siddhant Dogra|Meng Cao|Narges Razavian|David Kudlowitz|Lea Azour|William Moore|Yvonne Lui|Yindalon Aphinyanaphongs|Carlos Fernandez-Granda|Krzysztof J. Geras,Accept (Poster)
52,https://openreview.net/forum?id=ZVqjoKVbYMl,Multimodal Generative Learning on the MIMIC-CXR Database,"Machine Learning has become more and more popular in the medical domain over the past years. While supervised machine learning has already been applied successfully, the vast amount of unlabelled data offers new opportunities for un- and self-supervised learning methods. Especially with regard to the multimodal nature of most clinical data, the labelling of multiple data types becomes quickly infeasible in the medical domain. However, to the best of our knowledge, multimodal unsupervised methods have been tested extensively on toy-datasets only but have never been applied to real-world medical data, for direct applications such as disease classification and image generation. In this article, we demonstrate that self-supervised methods provide promising results on medical data while highlighting that the task is extremely challenging and that there is space for substantial improvements.",Hendrik J. Klug|Thomas M. Sutter|Julia E Vogt,Accept (Poster)
53,https://openreview.net/forum?id=rHAiz2pnxkB,$\mu$PEN: Multi-class PseudoEdgeNet for PD-L1 assessment,"In this paper, we take the recently presented PseudoEdgeNet model to the level of multi-class cell segmentation in histopathology images solely trained with point annotations. We tailor its loss function to the challenges of multi-class segmentation and equip it with an additional false positive loss term. We evaluate it on the assessment of tumor and immune cells in PD-L1 stained lung cancer histopathology images, and compare it with YOLOv5.",Jeroen Vermazeren|Leander van Eekelen|Luca Dulce Meesters|Monika Looijen-Salamon|Shoko Vos|Enrico Munari|Caner Mercan|Francesco Ciompi,Accept (Poster)
55,https://openreview.net/forum?id=HajxTQpPniD,Efficient biomedical image segmentation on Edge TPUs,"Biomedical semantic segmentation is typically performed on dedicated, costly hardware. In a recent study, we suggested an optimized, tiny-weight U-Net for an inexpensive hardware accelerator, the Google Edge TPU. Using an open biomedical dataset for high-speed laryngeal videoendoscopy, we exemplarily show that we can dramatically reduce the parameter space and computations while keeping a high segmentation quality. Using a custom upsampling routine, we fully deployed optimized architectures to the Edge TPU. Combining the optimized architecture and the Edge TPU, we gain a total speedup of >79$\times$ compared to our initial baseline while keeping a high accuracy. This combination allows to provide immediate results at the point of care, especially in constrained computational environments.",Andreas M Kist|Michael Döllinger,Accept (Poster)
59,https://openreview.net/forum?id=CSNQMsxteqm,Transformers for Ischemic Stroke Infarct Core Segmentation from Spatio-temporal CT Perfusion Scans,"The infarct core size is a crucial biomarker for treatment selection for ischemic stroke patients. For this purpose, we present a novel approach to perform infarct core segmentation using CT perfusion (CTP) source data, without ordinary deconvolution analysis. We propose the use of transformers to encode sequential CTP scans in spatial attention maps, which we subsequently use for infarct core segmentation. We report new top results on the ISLES 2018 challenge test data set for infarct core segmentation. This work presents a primary benchmark for infarct core segmentation from CTP source data using transformers.",Lucas de Vries|Bart Emmer|Charles Majoie|Henk Marquering|Efstratios Gavves,Accept (Poster)
62,https://openreview.net/forum?id=PLSdnHPx-W6,Deep ensemble model for segmenting microscopy images in the presence of limited labeled data,"Obtaining large amounts of high quality labeled microscopy data is expensive and time-consuming. To overcome this issue, we propose a deep ensemble model which aims to utilise limited labeled training data. We train multiple identical Convolutional Neural Network (CNN) segmentation models on training data that is partitioned into folds in two steps. First, the data is split based on sample diversity or expert knowledge reflecting the possible {\em modes} of the underlying data distribution. In the second step, these partitions are split into random folds like in a cross-validation setting. Segmentation models based on the U-net architecture are trained on each of these resulting folds yielding the candidate models for our deep ensemble model which are aggregated to obtain the final prediction. The proposed deep ensemble model is compared to relevant baselines, in their ability to segment interneurons in microscopic images of mice spinal cord, showing improved performance on an independent test set.",Jan Mikolaj Kaminski|Ilary Allodi|Roser Montañana-Rosell|Raghavendra Selvan|Ole Kiehn,Accept (Poster)
63,https://openreview.net/forum?id=ULm4D5bsiaE,TG-DGM: Clustering Brain Activity using a Temporal Graph Deep Generative Model,"Spatiotemporal graphs are a natural representation of dynamic brain activity derived from functional magnetic imaging (fMRI) data. Previous works, however, tend to ignore time dynamics of the brain and focus on static graphs. In this paper, we propose a temporal graph deep generative model (TG-DGM) which clusters brain regions into communities that evolve over time. In particular, subject embeddings capture inter-subject variability and its impact on communities using neural networks. We validate our model on the UK Biobank data. Results of up to 0.81 AUC ROC on the task of biological sex classification demonstrate that injecting time dynamics in our model outperforms a static baseline.",Simeon Emilov Spasov|Alexander Campbell|Giovana Dimitri|Alessandro Di Stefano|franco scarselli|Pietro Lio,Accept (Poster)
64,https://openreview.net/forum?id=7EZ4JOtlRl,Quality control of whole-slide images through multi-class semantic segmentation of artifacts,"Quality control is an integral part in the digitization process of whole-slide histopathology images due to artifacts that arise during various stages of slide preparation. Manual control and supervision of these gigapixel images are labor-intensive.  Therefore, we report the first multi-class deep learning model trained on whole-slide images covering multiple tissue and stain types for semantic segmentation of artifacts.  Our approach reaches a Dice score of 0.91, on average, across six artifact types, and outperforms the competition on external test set. Finally, we extend the artifact segmentation network to a multi-decision quality control system that can be deployed in routine clinical practice.",Gijs Smit|Francesco Ciompi|Maria Cigéhn|Anna Bodén|Jeroen van der Laak|Caner Mercan,Accept (Poster)
65,https://openreview.net/forum?id=0wblcjbC2sN,Interpretable Medical Image Classification with Self-Supervised Anatomical Embedding and Prior Knowledge,"In medical image analysis tasks, it is important to make machine learning models focus on correct anatomical locations, so as to improve interpretability and robustness of the model. We adopt a latest algorithm called self-supervised anatomical embedding (SAM) to locate point of interest (POI) on computed tomography (CT) scans. SAM can detect arbitrary POI with only one labeled sample needed. Then, we can extract targeted features from the POIs to train a simple prediction model guided by clinical prior knowledge. This approach mimics the practice of human radiologists, thus is interpretable, controllable, and robust. We illustrate our approach on the application of CT contrast phase classification and it outperforms an existing deep learning based method trained on the whole image.",Ke Yan|Youbao Tang|Adam P Harrison|Jinzheng Cai|Le Lu|Jingjing Lu,Accept (Poster)
67,https://openreview.net/forum?id=h7t0cFuX0m4,Recurrent Inference Machines as Inverse Problem Solvers for MR Relaxometry,"In this work, we propose the use of Recurrent Inference Machines (RIMs) to perform $T_1$ mapping. The RIM is a neural network framework that learns an iterative inference process using a model of the signal, similar to conventional statistical methods for quantitative MRI (QMRI), such as the Maximum Likelihood Estimator (MLE). Previously, RIMs were used to solve linear inverse reconstruction problems. Here, we show that they can also be used to optimize non-linear problems. The developed RIM framework is evaluated in terms of accuracy and precision and compared to an MLE method and an implementation of the ResNet. The results show that, compared to the other techniques in Monte Carlo experiments with simulated data, the RIM improves the precision of estimates without compromising in accuracy.",Emanoel Ribeiro Sabidussi|Stefan Klein|Matthan W. A. Caan|Shabab Bazrafkan|Arjan J. den Dekker|Jan Sijbers|Wiro Niessen|Dirk Poot,Accept (Poster)
68,https://openreview.net/forum?id=aGfL5C9wRx_,Test-Time Mixup Augmentation for Uncertainty Estimation in Skin Lesion Diagnosis,"Uncertainty is considered to be an important measure that provides valuable information on the learning behavior of deep neural networks. In this paper, we propose an uncertainty estimation method using test-time mixup augmentation (TTMA). The TTMA uncertainty is obtained by replacing affine augmentation with the mixup in the existing test-time augmentation (TTA) method. In addition to the data uncertainty, we propose TTMA-based class-specific uncertainty, which can provide information on between-class confusion. In experiments on the skin lesion diagnosis dataset, we confirmed that the proposed TTMA not only provides better epistemic uncertainty than TTA but also provides information on between-class confusion through class-specific uncertainty.",Hansang Lee|Haeil Lee|Helen Hong|Junmo Kim,Accept (Poster)
70,https://openreview.net/forum?id=CdQn5goh0E4,Comparison of CNN models on a multi-scanner database in colon cancer histology,"One of the most important challenges for computer-aided analysis in digital pathology is the development of robust deep neural networks, which can cope with variations in color and resolution of digitized whole-slide images (WSIs). It has been shown that color augmentation during training is a useful method to aid a model generalize better to heterogeneous data. In this work, we compare state of the art models EfficientNet, Xception, Inception, ResNet, DenseNet, MobileNet and QuickNet on a multi-scanner database comprising slides each digitized with six different scanners. All of the networks are trained with data of only one scanner applying a combination of color and blur augmentation techniques. All models show similar tendencies across the different scanner databases but differ in the overall classification accuracy. Differences in training and inference time, however, are more pronounced: on a mid-range GPU, the inference time of the fastest model (QuickNet) is 13 times faster than the slowest one (EfficientNet B4). There is also a trade-off between speed and accuracy, the slower networks are more stable across different scanners and show the overall best performance. A good compromise between quality and inference time is achieved by EfficientNet B0.",Petr Kuritcyn|Michaela Benz|Jakob Dexl|Volker Bruns|Arndt Hartmann|Carol Geppert,Accept (Poster)
73,https://openreview.net/forum?id=hPUnpHJHuy,Learning to Represent Whole Slide Images by Selecting Cell Graphs of Patches,"Advances in multiplex biomarker imaging systems have enabled the study of complex spatial biology within the tumor microenvironment. However, the high-resolution multiplexed images are often only available for a subset of regions of interest (RoIs), clinical data is not easily accessible and the datasets are generally too small to apply off-the-shelf deep learning methods commonly used in histopathology. In this paper, we focus on datasets with few images and without labels, and aim to learn representations for slides. We choose a task of patient identification that leads our new model to select RoIs with discriminative properties and infer patient-specific features that can be used later for any task via transfer learning. The experimental results on the synthetic data generated by taking the tumor microenvironment into account indicate that the proposed method is a promising step towards computer-aided analysis in unlabeled datasets of high-resolution images.",Yinan Zhang|Beril Besbinar|Pascal Frossard,Accept (Poster)
76,https://openreview.net/forum?id=tv_pkmFzdC,Robust medical image segmentation by adapting neural networks for each test image,"Performance of convolutional neural networks (CNNs) used for medical image analyses degrades markedly when training and test images differ in terms of their acquisition details, such as the scanner model or the protocol. We tackle this issue for the task of image segmentation by adapting a CNN ($C$) for each test image. Specifically, we design $C$ as a concatenation of a shallow normalization CNN ($N$), followed by a deep CNN ($S$) that segments the normalized image. At test time, we adapt $N$ for each test image, guided by an implicit prior on the predicted labels, which is modelled using an independently trained denoising autoencoder ($D$). The method is validated on multi-center MRI datasets of 3 anatomies. This article is a short version of the journal paper~\cite{karani2021test}.",Neerav Karani|Ertunc Erdil|Krishna Chaitanya|Ender Konukoglu,Accept (Poster)
80,https://openreview.net/forum?id=W9sz0zHk33h,Prediction of Ki67 scores from H&E stained breast cancer sections using convolutional neural networks,"Ki67 is an established marker of proliferation in breast cancer, but currently has limited clinical value due to limitations of the analytical validity of immunohistochemistry (IHC) -based Ki67 scoring. While the inter-assessor variability of scoring can be improved through image analysis software, Ki67 IHC also suffers from a lack of standardized staining protocols and is not part of routine pathology workflow in most countries. This could potentially be alleviated through directly predicting Ki67 scores from routine hematoxylin and eosin (H\&E) stained whole-slide-images (WSIs). We compared four different deep learning based approaches to  predict Ki67 scores from routine H\&E stained WSIs in a dataset that consists of matched H\&E and Ki67 WSIs from 126 breast cancer patients, resulting in a Spearman correlation between WSI cancer ROI averages of 0.546 for the best performing model in a 5-fold cross-validation (CV). These findings suggest that it is possible to predict the Ki67 score from H\&E stained WSIs, but validation in a larger cohort is required to meaningfully distinguish the performance of the methods that were investigated. ",Philippe Weitz|Balazs Acs|Johan Hartman|Mattias Rantalainen,Accept (Poster)
86,https://openreview.net/forum?id=XT40FwD5bV,Comparison of Representation Learning Techniques for Tracking in time resolved 3D Ultrasound,"3D ultrasound (3DUS) becomes more interesting for target tracking in radiation therapy due to its capability to provide volumetric images in real-time without using ionizing radiation. It is potentially usable for tracking without using fiducials. For this, a method for learning meaningful representations with which recognizing anatomical structures in different time frames is capable would be useful. In this study, 3DUS patches are reduced into a 128-dimensional representation space using conventional autoencoder, variational autoencoder and sliced-wasserstein autoencoder. In the representation space, the capability of separating different ultrasound patches as well as recognizing similar patches is investigated and compared based on a dataset of liver images. Two metrics to evaluate the tracking capability in the representation space are proposed. It is shown that ultrasound patches with different anatomical structures can be distinguished and sets of similar patches can be clustered in representation space. The results indicate that the investigated autoencoders have different levels of usability for target tracking in 3DUS.",Daniel Wulff|Jannis Hagenah|Floris Ernst,Accept (Poster)
89,https://openreview.net/forum?id=-KI5qmKvhKQ,Cine-MRI detection of abdominal adhesions with spatio-temporal deep learning,"Adhesions are an important cause of chronic pain following abdominal surgery. Recent developments in abdominal cine-MRI have enabled the non-invasive diagnosis of adhesions. Adhesions are identified on cine-MRI by the absence of sliding motion during movement. Diagnosis and mapping of adhesions  improves the management of patients with pain. Detection of abdominal adhesions on cine-MRI is challenging from both a radiological and deep learning perspective. We focus on classifying presence or absence of adhesions in sagittal abdominal cine-MRI series. We experimented with spatio-temporal deep learning architectures centered around a ConvGRU architecture. A hybrid architecture comprising a ResNet followed by a ConvGRU model allows to classify a whole time-series. Compared to a stand-alone ResNet with a two time-point (inspiration/expiration) input, we show an increase in classification performance (AUROC) from 0.74 to 0.83 ($p<0.05$). Our full temporal classification approach adds only a small amount (5%) of parameters to the entire architecture, which may be useful for other  medical imaging problems with a temporal dimension. ",Bram de Wilde|Richard P. G. ten Broek|Henkjan Huisman,Accept (Poster)
93,https://openreview.net/forum?id=wiKDehhdnz,Synthesis of Diabetic Retina Fundus Images Using Semantic Label Generation,"Automatic segmentation of retina lesions have been a long standing and challenging task for learning based models, mostly due to the lack of available and accurate lesion segmentation datasets. In this paper, we propose a two-step process for generating photo-realistic fundus images conditioned on synthetic ""ground truth"" semantic labels, and demonstrate its potential for further downstream tasks, such as, but not limited to; automated grading of diabetic retinopathy, dataset balancing, creating image examples for trainee ophthalmologists, etc.",Joon-Ho Son|Amir Alansary|Daniel Rueckert|Bernhard Kainz|Benjamin Hou,Accept (Poster)
94,https://openreview.net/forum?id=-j7vnPsPWys,Abnormality Detection in Histopathology via Density Estimation with Normalising Flows,Diagnosis of cancer often relies on the time-consuming examination of histopathology slides by expert pathologists. Automation via supervised deep learning methods require large amounts of pixel-wise annotated data that is costly to acquire. Unsupervised density estimation methods that rely only on the availability of healthy examples could cut down the cost of annotation. We propose to use residual flows as density estimator and compare different tests for out-of-distribution (OOD) detection. Our results suggest that unsupervised OOD detection is a viable approach for detecting suspicious regions in histopathology slides.,Nick Pawlowski|Ben Glocker,Accept (Poster)
96,https://openreview.net/forum?id=0bpkIn63sNG,Semi-Supervised Siamese Network for Identifying Bad Data in Medical Imaging Datasets,"Noisy data present in medical imaging datasets can often aid the development of robust models that are equipped to handle real-world data. However, if the bad data contains insufficient anatomical information, it can have a severe negative effect on the model's performance. We propose a novel methodology using a semi-supervised Siamese network to identify bad data. This method requires only a small pool of 'reference' medical images to be reviewed by a non-expert human to ensure the major anatomical structures are present in the Field of View. The model trains on this reference set and identifies bad data by using the Siamese network to compute the distance between the reference set and all other medical images in the dataset. This methodology achieves an Area Under the Curve (AUC) of 0.989 for identifying bad data. Code will be available at https://git.io/JYFuV.",Niamh Belton|Kathleen M Curran|Aonghus Lawlor,Accept (Poster)
98,https://openreview.net/forum?id=fanGydarIPF,3D Scout Scans Using Projection Domain Denoising,"Low dose 2D scouts, also known as topograms, are commonly used for CT scan planning. Although 3D CT volumes can provide more valuable information for the selection of the scan range and parameters, the very low X-ray dose used during scout scan acquisitions results in artefacts requiring effective denoising techniques to make them useful. This has proved challenging for traditional denoising algorithms. We propose a projection domain denoiser based on a convolutional neural network (CNN), which provides improved image quality even at ultra-low dose levels. We compare two CNNs operating on two data representations, a conventional line integral data and raw photon counts, which have different quantitative properties and dynamic ranges. The results show that the two denoising strategies give rise to different properties of reconstructed images and that both projection CNNs are effective for ultra-low dose CT denoising.",Mikhail Bortnikov|Frank Bergner|Alexey Chernyavskiy|Kevin M. Brown|Noel Black|Michael Grass,Accept (Poster)
99,https://openreview.net/forum?id=cA4VVWbNO-,Strength in Diversity: Understanding the impacts of diverse training sets in self-supervised pre-training for histology images,"Self-supervised learning (SSL) has demonstrated success in computer vision tasks for natural images, and recently histopathological images, where there is limited availability of annotations. Despite this, there has been limited research into how the diversity of source data used for SSL tasks impacts performance. The current study quantifies changes to downstream classification of metastatic tissue in lymph node sections of the PatchCamelyon dataset when datasets from different domains (natural images, textures, histology) are used for SSL pre-training. We show that for cases with limited training data, using diverse datasets from different domains for SSL pre-training can achieve comparable performance when compared with SSL pre-training on the target dataset.",Kristina Lynn Kupferschmidt|Eu Wern Teh|Graham W. Taylor,Accept (Poster)
101,https://openreview.net/forum?id=xqZTapYnEcG,Creating Anthropomorphic Phantoms via Unsupervised Convolutional Neural Networks,"Computerized phantoms play an important role in medical imaging research. They can serve as a gold standard for evaluating and optimizing medical imaging analysis, processing, and reconstruction methods. Existing computerized phantoms model anatomical variations through organ and phantom scaling, which does not fully capture the range of anatomical variations seen in humans. Here, we present a registration-based method for creating highly realistic and detailed anthropomorphic phantoms. The proposed registration method is built on the use of an unsupervised convolutional neural network (ConvNet) that warps the four-dimensional Xtended Cardiac-Torso (XCAT) phantom to a patient CT scan. The registration ConvNet iteratively optimizes an SSIM-based loss function for a given image pair without prior training. We experimentally show substantially improved image similarity of the generated phantom using the proposed method to a patient image.",Junyu Chen|Ye Li|Yong Du|Eric Frey,Accept (Poster)
106,https://openreview.net/forum?id=M1VznPOV5jV,Scopeformer: n-CNN-ViT hybrid model for Intracranial hemorrhage subtypes classification,"We propose a feature generator backbone composed of an ensemble of convolutional neural networks (CNNs) to improve the recently emerging Vision Transformer (ViT) models. We tackled the RSNA intracranial hemorrhage classification problem, i.e., identifying various hemorrhage  types  from  computed  tomography  (CT)  slices.   We  show  that  by gradually stacking  several  feature  maps  extracted  using  multiple  Xception  CNNs,  we  can develop a  feature-rich  input  for  the  ViT  model.   Our  approach  allowed  the  ViT  model  to  pay attention to relevant features at multiple levels.  Moreover, pretraining the ”n” CNNs using various paradigms leads to a diverse feature set and further improves the performance of the proposed n-CNN-ViT. We achieved a test accuracy of 98.04% with a weighted logarithmic loss value of 0.0708.  The proposed architecture is modular and scalable in both the number of CNNs used for feature extraction and the size of the ViT.",Yassine Barhoumi|Ghulam Rasool,Accept (Poster)
108,https://openreview.net/forum?id=XHWqF4DlRr0,Efficient and Accurate Spatial-Temporal Denoising Network for Low-dose CT Scans,"While deep-learning-based imaging denoising techniques can improve the quality of low-dose computed tomography (CT) scans, repetitive 3D convolution operations cost significant computation resources and time. We present an efficient and accurate spatial-temporal convolution method to accelerate an existing denoising network based on the SRResNet. We trained and evaluated our model on our dataset containing 184 low-dose chest CT scans. We compared the performance of the proposed spatial-temporal convolution network to the SRResNet with full 3D convolutional layers. Using 8-bit quantization, we demonstrated a 7-fold speed-up during inference. Using lung nodule characterization as a driving task, we analyzed the impact on image quality and radiomic features. Our results show that our method achieves better perceptual quality, and the outputs are consistent with the SRResNet baseline outputs for some radiomics features (31 out of 57 total features). These observations together demonstrate that the proposed spatial-temporal method can be potentially useful for clinical applications where the computational resource is limited. ",Leihao Wei|William Hsu,Accept (Poster)
109,https://openreview.net/forum?id=2t0_AxD1otB,"DS6, Deformation-aware Semi-supervised Learning: Application to Small Vessel Segmentation with Noisy Training Data","The advancement of 7 Tesla MRI systems enabled the depiction of very small vessels in the brain. Segmentation and quantification of the small vessels in the brain is a critical step in the study of Cerebral Small Vessel Disease, which is a challenging task. This paper proposes a deep learning based on U-Net Multi-Scale Supervision architecture to automatically segment small vessels in 7 Tesla 3D Time-of-Flight (TOF) Magnetic Resonance Angiography (MRA) data trained on a small imperfect semi-automatically segmented dataset and was made equivariant to elastic deformations in a self-supervised manner using deformation-aware learning to improve the generalisation performance. The proposed method achieved a dice score of 80.44$\pm$0.83 while being compared against the semi-automatically created labels and 62.07 while comparing against manually segmented region. ",Soumick Chatterjee|Kartik Prabhu|Mahantesh Pattadkal|Gerda Bortsova|Chompunuch Sarasaen|Florian Dubost|Hendrik Mattern|Marleen de Bruijne|Oliver Speck|Andreas Nürnberger,Accept (Poster)
110,https://openreview.net/forum?id=KNEKu-W4Avz,ReconResNet: Regularised Residual Learning for MR Image Reconstruction of Undersampled Cartesian and Radial Data,"MRI is an inherently slow process, which leads to long scan time for high-resolution imaging. The speed of acquisition can be increased by ignoring parts of the data (undersampling). Consequently, this leads to the degradation of image quality. This work proposes a deep learning based MRI reconstruction framework to reconstruct highly undersampled Cartesian or radial MR acquisitions, which includes a modified regularised version of ResNet as the network backbone to remove artefacts from the undersampled image, followed by data consistency steps that fusions the network output with the data already available from undersampled k-space in order to further improve reconstruction quality. The performance of this framework for various undersampling patterns has also been tested, and it has been observed that the framework is robust to deal with various sampling patterns - results in very high quality reconstruction (highest SSIM being 0.990$\pm$0.006 for acceleration factor of 3.5), while being compared with the fully sampled reconstruction. It has been shown that the proposed framework can successfully reconstruct even for an acceleration factor of 20 for Cartesian (0.968$\pm$0.005) and 17 for radially (0.962$\pm$0.012) sampled data. ",Soumick Chatterjee|Mario Breitkopf|Chompunuch Sarasaen|Hadya Yassin|Georg Rose|Andreas Nürnberger|Oliver Speck,Accept (Poster)
112,https://openreview.net/forum?id=V4k0rNW7IG-,Weakly supervised 3D ConvLSTMs for high precision Monte-Carlo radiotherapy dose simulations,"Radiotherapy dose simulation using the Monte-Carlo technique surpasses existing algorithms in terms of precision but remains too time-consuming to be integrated in clinical workflows. We introduce a 3D recurrent and fully convolutional neural network architecture to produce high-precision Monte-Carlo-like dose simulations from low-precision and cheap-to-compute ones. We use the noise-to-noise setting, a weakly supervised training strategy, by training the models solely on low-precision data without expensive-to-compute, high-precision dose simulations. Several evaluation metrics are used to compare with other methods and to assess the clinical viability and quality of the generated dose maps.",Sonia Martinot|Norbert Bus|Maria Vakalopoulou|charlotte robert|Eric Deutsch|Nikos Paragios,Accept (Poster)
113,https://openreview.net/forum?id=Tz_X8xpgYsO,Ex-vivo - to - In-vivo Learning in Cardiology,"The clinical Atrial Fibrillation (AF) visualization method, multi-electrode mapping (MEM), delivers electrode grid $\textit{in-vivo}$ to the heart muscle and is known for its low resolution. A more cutting-edge imaging modality, near-infrared optical mapping (NIOM), allows seeing the AF sources in high resolution; however, it is currently $\textit{ex-vivo}$ only (i.e., designed for explanted organs only). In this work, we present the $\textit{ex-vivo}$ to the $\textit{in-vivo}$ learning paradigm, where the former serves the purpose of improving the latter. Specifically, the NIOM improves the detection of AF sources in MEM data via an image-to-image model. We validate the idea on 7 explanted human hearts and test the models on 2 clinical cases.",Alexander M. Zolotarev|Oleg Y. Rogov|Aleksei V. Mikhailov|John D. Hummel|Vadim V Fedorov|Dmitry V. Dylov,Accept (Poster)
117,https://openreview.net/forum?id=jgBzGIG-kB,Cycle Consistent Embedding of 3D Brains with Auto-Encoding Generative Adversarial Networks,"Modern generative adversarial networks (GANs) have been enabling the realistic generation of full 3D brain images by sampling from a latent space prior $\mathcal{Z}$ (i.e., random vectors) and mapping it to realistic images in $\mathcal{X}$ (e.g., 3D MRIs). To address the ubiquitous mode collapse issue, recent works have strongly imposed certain characteristics such as Gaussianness to the prior by also explicitly mapping $\mathcal{X}$ to $\mathcal{Z}$ via encoder. These efforts, however, fail to accurately map 3D brain images to the desirable prior, which the generator assumes to be sampling the random vectors from. On the other hand, Variational Auto-Encoding GAN (VAE-GAN) solves mode collapse by enforcing Gaussianness by two learned parameter, yet causes blurriness in images. In this work, we show how our \textit{cycle consistent embedding} GAN (CCE-GAN) both accurately encodes 3D MRIs to the standard normal prior, and maintains the quality of the generated images. We achieve this without a network-based code discriminator via the Wasserstein measure. We quantitatively and qualitatively assess the embeddings and the generated 3D MRIs using healthy T1-weighted MRIs from ADNI.",Shibo Xing|Harsh Sinha|Seong Jae Hwang,Accept (Poster)
120,https://openreview.net/forum?id=tgkEqYyA12p,Self-supervised Visual Place Recognition for Colonoscopy Sequences,"We present the first place recognition system trained specifically for colonoscopy sequences. We use the convolutional neural network for image retrieval proposed by Radenovic et al. and we fine-tune it using image pairs from real human colonoscopies. The colonoscopy frames are clustered automatically by a Structure-from-Motion (SfM) algorithm, which has proven to cope with scene deformation and illumination changes. The experiments show that the system is able to generalize by testing in a different human colonoscopy, retrieving frames observing the same place despite of the different viewpoint and illumination changes. The proposed place recognition would be a key component of Simultaneous Localization and Mapping (SLAM) systems operating in colonoscopy to assist doctors during the explorations or to support robotization. 
",Javier Morlana|Pablo Azagra Millán|Javier Civera|José M. M. Montiel,Accept (Poster)
121,https://openreview.net/forum?id=_BIpUlEB6ff,A hybrid model- and deep learning-based framework for functional lung image synthesis from non-contrast multi-inflation CT,"Hyperpolarized gas MRI can visualize and quantify regional lung ventilation with exquisite detail but requires highly specialized equipment and exogenous contrast. Alternative, non-contrast techniques, including CT-based models of ventilation have shown moderate spatial correlations with hyperpolarized gas MRI. Here, we propose a hybrid framework that integrates CT-ventilation modelling and deep learning approaches. The hybrid model/DL framework generated synthetic ventilation images which accurately replicated gross ventilation defects in hyperpolarized gas MRI scans, significantly outperforming other model- and DL-only approaches. Our results show that a synergy between conventional CT-ventilation modelling and DL can improve the performance of functional lung image synthesis.",Joshua Russell Astley|Alberto M Biancardi|Helen Marshall|Guilhem J Collier|Paul JC Hughes|Jim M Wild|Bilal A Tahir,Accept (Poster)
122,https://openreview.net/forum?id=zZA5TpNdC4Z,Rethinking the Design of Learning based Inter-Patient Registration using Deformable Supervoxels ,"Deep learning has the potential to substantially improve inter-subject alignment for shape and atlas analysis. So far most highly accurate supervised approaches require dense manual annotations and complex multi-level architectures but may still be susceptible to label bias. We present a radically different approach for learning to estimate large deformations without expert supervision. Instead of regressing displacements, we train a 3D DeepLab network to predict automatic supervoxel segmentations. To enable consistent supervoxel labels, we use the warping field of a conventional approach and increase the accuracy by sampling multiple complementary over-segmentations. We experimentally demonstrate that 1) our deformable supervoxels are less sensitive to large initial misalignment and can combine linear and nonlinear registration and 2) using this self-supervised classification loss is more robust to noisy ground truth and leads to better convergence than direct regression as supervision.",Mattias P Heinrich,Accept (Poster)
125,https://openreview.net/forum?id=GOhAojdaLg,Semi-supervised Image-to-Image translation for robust image registration,"The Japan Brain/MINDS Project aims at studying the neural networks controlling higher brain functions in the marmoset. As part of it, we develop an image processing pipeline for marmoset brain imaging data, where various microscopy images of different modalities need to be co-registered. In initial experiments, multi-modal image registration frequently failed due to an erroneous initialization. Our data set includes images of Nissl stained brain sections, backlit images as well as images of neural tracer injections using two-photon microscopy. More than 10000 high-resolution 2D images required co-registration, a large amount that demands a reliable automation process. We implemented a semi-supervised image-to-image translation which allowed a robust image alignment initialization. With such an initial alignment, all images can be successfully registered using a state-of-the-art multi-modal image registration algorithm.",henrik skibbe|akiya watakabe|Febrian Rachmadi|Carlos Enrique Gutierrez|Ken Nakae|tetsuo yamamori,Accept (Poster)
126,https://openreview.net/forum?id=ImcP8kkqtfZ,Radiographic Assessment of CVC Malpositioning: How can AI best support clinicians?,"The malpositioning of central venous catheters (CVCs) is a common technical complication that is usually diagnosed on post-procedure chest X-rays (CXRs). If the misplaced CVC remains undetected, it can lead to serious health consequences for the patient. Interpreting CXRs at a large scale in everyday clinical practice is time consuming and can create delays in the repositioning of the CVC. A computer-assisted assessment of post-procedure CXRs can help to prioritise cases and reduce human errors in stressful situations by objectifying decisions. However, final assessments must always be made by the clinicians, which is why the algorithmic support needs to be comprehensible.
Since AI models are not yet able to detect catheter maplpositons with highest accuracy, the focus must be on efficient support in everyday clinical practice. In this work, we evaluate three different AI models, particularly with regard to the relationship between classification accuracy and the degree of explainability. Our results show how helpful it is to incorporate explicit clinical knowledge into deep learning-based models and give us promising research directions for a planned large scale patient study.",Lasse Hansen|Malte Sieren|Malte Hobe|Axel Saalbach|Heinrich Schulz|Jörg Barkhausen|Mattias P Heinrich,Accept (Poster)
128,https://openreview.net/forum?id=sua3vlnkmEv,Learning a Metric without Supervision: Multimodal Registration using Synthetic Cycle Discrepancy,"Training deep learning based medical image registration methods involves the challenge of finding a suitable metric. To avoid the difficulty of choosing a metric for multimodal image registration, we propose a completely new concept relying on geometric instead of metric supervision with three-way registration cycles. Therefore, we create a synthetic image by applying a synthetic transformation on one of the input images. This leads to cycles that for each pair of input images comprise two multimodal transformations to be estimated and one known synthetic monomodal transformation. We minimise the discrepancy between the combined multimodal transformations and the synthetic monomodal transformation. By minimising this cycle discrepancy, we are able to learn multimodal registration between CT and MRI without metric supervision. Our method outperforms state-of-the-art metric supervision and comes very close to fully-supervised learning with ground truth labels. ",Hanna Siebert|Lasse Hansen|Mattias P Heinrich,Accept (Poster)
133,https://openreview.net/forum?id=1JP1g5htY6K,Virtual Bone Shape Aging,"We use deep learning to age knee bone surfaces four years. We propose to encode an MRI-based bone surface in a spherical coordinate format, and use these spherical maps to predict shape changes in a 48 months time frame, in subjects with and without osteoarthritis. The experiments show that a 2D V-Net can predict bone surface shape with a mean absolute error of about 1 mm. Our code is available  at https://github.com/fcaliva/Bone_Shape_Virtual_Aging.",Francesco Caliva|Alejandro Morales Martinez|Sharmila Majumdar|Valentina Pedoia,Accept (Poster)
138,https://openreview.net/forum?id=IAuBCvaTKHr,Breast cancer patient stratification using domain adaptation based lymphocyte detection in HER2 stained tissue sections,"We extend the CycleGAN architecture with a style-based generator and show the efficacy of the proposed domain adaptation-based method between two histopathology image domains - Hematoxylin and Eosin (H&E) and HER2 immunohistochemically (IHC) images. Using the proposed method, we re-used large set of pre-existing annotations for detection of tumor infiltrating lymphocytes (TILs), which were originally done on H&E, towards a TIL detector applicable on HER2 IHC images. We provide analytical validation of the resulting TIL detector. Furthermore, we show that the detected stromal TIL densities are significantly prognostic as a biomarker for patient stratification on a triple-negative breast cancer (TNBC) cohort.
",Ansh Kapil|Armin Meier|Anatoliy Shumilov|Susanne Haneder|Helen Angell|Günter Schmidt,Accept (Poster)
141,https://openreview.net/forum?id=yVYVzsNWvN,Optimizing Operating Points for High Performance Lesion Detection and Segmentation Using Lesion Size Reweighting,"There are many clinical contexts which require accurate detection and segmentation of all focal pathologies (e.g. lesions, tumours) in patient images. In cases where there are a mix of small and large lesions, standard binary cross entropy loss will result in better segmentation of large lesions at the expense of missing small ones. Adjusting the operating point to accurately detect all lesions generally leads to oversegmentation of large lesions. In this work, we propose a novel reweighing strategy to eliminate this performance gap, increasing small pathology detection performance while maintaining segmentation accuracy. We show that our reweighing strategy vastly outperforms competing strategies based on experiments on a large scale, multi-scanner, multi-center dataset of Multiple Sclerosis patient images.",Brennan Nichyporuk|Justin Szeto|Douglas Arnold|Tal Arbel,Accept (Poster)
144,https://openreview.net/forum?id=fQDGt0RJkMu,Gated CNNs for Nuclei Segmentation in H&E Breast Images,"Nuclei segmentation using deep learning has been achieving high accuracy using U-Net and variants, but a remaining challenge is distinguishing touching and overlapping cells. In this work, we propose using gated CNN (GCNN) networks to obtain sharper predictions around object boundaries and improve nuclei segmentation performance. The method is evaluated in over 1000 multicentre diverse H&E breast cancer images from three databases and compared to baseline U-Net and R2U-Net.",Shana Beniamin|April Khademi|Dimitri Androutsos,Accept (Poster)
145,https://openreview.net/forum?id=vNPQTZfPjFO,Reconstruction and coil combination of undersampled concentric-ring MRSI data using a Graph U-Net,"Geometric deep learning has recently gained influence, as it allows the extension of convolutional neural networks to non euclidean domains. In this paper graph neural networks (GNNs) are used for reconstruction and coil combination of undersampled concentric-ring k-space MRSI data. We show that graph U-nets perform better on undersampled data than GNNs. Specifically, results suggest that the omission of self-connecting edges results in a more stable behavior and better training for graph U-nets.",Paul Weiser|Stanislav Motyka|Georg Langs|Wolfgang Bogner,Accept (Poster)
