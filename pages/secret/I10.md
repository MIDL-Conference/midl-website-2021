---
title: "Partial transfusion: on the expressive influence of trainable batch norm parameters for transfer learning"
page_class: "paper"
---

{% from "_macros.html" import html_presentation, button, teaser, video %}

# I10 - Partial transfusion: on the expressive influence of trainable batch norm parameters for transfer learning

#### Fahdi Kanavati, Masayuki Tsuneki

[% .details %]
<a class="toggle_visibility" data-selector=".abstract" data-level="3">Show abstract</a>
- <a class="toggle_visibility" data-selector=".schedule" data-level="3">Show schedule</a>
- <a href="">Proceedings</a>
- <a href="https://openreview.net/pdf?id=TjwDWRdfZpg">PDF</a>
- <a href="https://openreview.net/forum?id=TjwDWRdfZpg">Reviews</a>
{{ teaser('') }}

<p>
    <span class="abstract">
        Transfer learning from ImageNet is the go-to approach when applying deep learning to medical images. The approach is either to fine-tune a pre-trained model or use it as a feature extractor.Most modern architecture contain batch normalisation layers, and fine-tuning a model with such layers requires taking a few precautions as they consist of trainable and non-trainable weights and have two operating modes: training and inference. Attention is primarily given to the non-trainable weights used during inference, as they are the primary source of unexpected behaviour or degradation in performance during transfer learning. It is typically recommended to fine-tune the model with the batch normalisation layers kept in inference mode during both training and inference. In this paper, we pay closer attention instead to the trainable weights of the batch normalisation layers, and we explore their expressive influence in the context of transfer learning.We find that only fine-tuning the trainable weights (scale and centre) of the batch normalisation layers leads to similar performance as to fine-tuning all of the weights, with the added benefit of faster convergence. We demonstrate this on a variety of seven publicly available medical imaging datasets, using four different model architectures. 
        <br>
        <span class="actions"><a class="toggle_visibility" data-level="2">Hide abstract</a></span>
    </span>
</p>

<p>
    <span class="schedule">
         Friday 9th July<br>I4-12 (short): Transfer Learning and Domain Adaptation - 13:45 - 14:30 (UCT+2)
        <br>
        <span class="actions"><a class="toggle_visibility" data-level="2">Hide schedule</a></span>
    </span>
</p>

[% / %]


---

{{ html_presentation('https://midl2021.kervadec.science/contents/full_41_video.mp4', 'https://midl2021.kervadec.science/contents/full_41_poster.pdf', 720, 450) }}