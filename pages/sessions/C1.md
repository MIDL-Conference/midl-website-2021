---
title: "Session C1"
page_class: "program"
sidebar: false
---

{% from "_macros.html" import paper, cloudflare_video %}


# C1 - Endoscopy and Validation Studies
##  Wednesday 7th July, 16:45 - 17:30 (UCT+2)
### Chairs: Sandy Engelhardt, Lena Maier-Hein



---
                        
[% .papers %]
{{ paper('Efficient biomedical image segmentation on Edge TPUs',
        'Andreas M Kist, Michael Döllinger',
        openreview='https://openreview.net/forum?id=HajxTQpPniD',
        pdf='https://openreview.net/pdf?id=HajxTQpPniD',
        id='C1',
        paper='papers/C1.html',
        proceedings='',
        abstract='Biomedical semantic segmentation is typically performed on dedicated, costly hardware. In a recent study, we suggested an optimized, tiny-weight U-Net for an inexpensive hardware accelerator, the Google Edge TPU. Using an open biomedical dataset for high-speed laryngeal videoendoscopy, we exemplarily show that we can dramatically reduce the parameter space and computations while keeping a high segmentation quality. Using a custom upsampling routine, we fully deployed optimized architectures to the Edge TPU. Combining the optimized architecture and the Edge TPU, we gain a total speedup of >79x compared to our initial baseline while keeping a high accuracy. This combination allows to provide immediate results at the point of care, especially in constrained computational environments.')
}}
[% / %]
                        
{{ cloudflare_video('8f2cdc3741b1a9da54d0ad597a4ebe48') }}
                        

---
                        
[% .papers %]
{{ paper('Deep ensembles based on Stochastic Activation Selection for Polyp Segmentation',
        'Alessandra Lumini, Loris Nanni, Gianluca Maguolo',
        openreview='https://openreview.net/forum?id=NJcszyl19PN',
        pdf='https://openreview.net/pdf?id=NJcszyl19PN',
        id='C2',
        paper='papers/C2.html',
        proceedings='',
        abstract='Semantic segmentation has a wide array of applications ranging from medical-image analysis, scene understanding,\xa0autonomous driving and robotic navigation. This work deals with medical image segmentation and in particular with accurate polyp detection and segmentation during colonoscopy examinations. Several convolutional neural network architectures have been proposed to effectively deal with this task and with the problem of segmenting objects at different scale input. The basic architecture in image segmentation consists of an encoder and a decoder: the first uses convolutional filters to extract features from the image, the second is responsible for generating the final output. In this work, we compare some variant of the DeepLab architecture obtained by varying the decoder backbone. We compare several decoder architectures, including ResNet, Xception, EfficentNet, MobileNet and we perturb their layers by substituting ReLU activation layers with other functions. The resulting methods are used to create deep ensembles which are shown to be very effective. Our experimental evaluations show that our best ensemble produces good segmentation results by achieving high evaluation scores with a dice coefficient of  0.884, and a mean Intersection over Union (mIoU) of 0.818 for the Kvasir-SEG dataset. To improve reproducibility and research efficiency the MATLAB source code used for this research is available at GitHub: https://github.com/LorisNanni.')
}}
[% / %]
                        
{{ cloudflare_video('143be009291dc94dc5c41d6f3f96849b') }}
                        

---
                        
[% .papers %]
{{ paper('Self-supervised Visual Place Recognition for Colonoscopy Sequences',
        'Javier Morlana, Pablo Azagra Millán, Javier Civera, José M. M. Montiel',
        openreview='https://openreview.net/forum?id=tgkEqYyA12p',
        pdf='https://openreview.net/pdf?id=tgkEqYyA12p',
        id='C3',
        paper='papers/C3.html',
        proceedings='',
        abstract='We present the first place recognition system trained specifically for colonoscopy sequences. We use the convolutional neural network for image retrieval proposed by Radenovic et al. and we fine-tune it using image pairs from real human colonoscopies. The colonoscopy frames are clustered automatically by a Structure-from-Motion (SfM) algorithm, which has proven to cope with scene deformation and illumination changes. The experiments show that the system is able to generalize by testing in a different human colonoscopy, retrieving frames observing the same place despite of the different viewpoint and illumination changes. The proposed place recognition would be a key component of Simultaneous Localization and Mapping (SLAM) systems operating in colonoscopy to assist doctors during the explorations or to support robotization. ')
}}
[% / %]
                        
{{ cloudflare_video('f3d56c5b1438ffa7e02cb8c6e6ea9506') }}
                        

---
                        
[% .papers %]
{{ paper('Carbon footprint driven deep learning model selection for medical imaging',
        'Raghavendra Selvan',
        openreview='https://openreview.net/forum?id=1TPRpNyyj2L',
        pdf='https://openreview.net/pdf?id=1TPRpNyyj2L',
        id='C4',
        paper='papers/C4.html',
        proceedings='',
        abstract='Selecting task appropriate deep learning models is a resource intensive process; more so when working with large quantities of high dimensional data that are encountered in medical imaging. Model selection procedures that are primarily aimed at improving performance measures such as accuracy could become biased towards resource intensive models. In this work, we propose to inform and drive the model selection procedure using the carbon footprint of training deep learning models as a complementary measure along with other standard performance metrics. We experimentally demonstrate that increasing carbon footprint of large models might not necessarily translate into proportional performance gains, and suggest useful trade-offs to obtain resource efficient models.')
}}
[% / %]
                        
{{ cloudflare_video('a5ccb8108ec5c6e27b42b857441cef99') }}
                        

---
                        
[% .papers %]
{{ paper('SWNet: Surgical Workflow Recognition with Deep Convolutional Network',
        'Bokai Zhang, Amer Ghanem, Alexander Simes, Henry Choi, Andrew Yoo, Andrew Min',
        openreview='https://openreview.net/forum?id=g1sESqlP214',
        pdf='/proceedings/zhang21c.pdf',
        id='C5',
        paper='papers/C5.html',
        proceedings='',
        abstract='Surgical workflow recognition has been playing an essential role in computer-assisted interventional systems for modern operating rooms. In this paper, we present a computer vision-based method named SWNet that focuses on utilizing spatial information and temporal information from the surgical video to achieve surgical workflow recognition. As the first step, we utilize Interaction-Preserved Channel-Separated Convolutional Network (IP-CSN) to extract features that contain spatial information and local temporal information from the surgical video through segments. Secondly, we train a Multi-Stage Temporal Convolutional Network (MS-TCN) with those extracted features to capture global temporal information from the full surgical video. Finally, by utilizing Prior Knowledge Noise Filtering (PKNF), prediction noise from the output of MS-TCN is filtered. We evaluate SWNet for Sleeve Gastrectomy surgical workflow recognition. SWNet achieves 90% frame-level accuracy and reaches a weighted Jaccard Score of 0.8256. This demonstrates that SWNet has considerable potential to solve the surgical workflow recognition problem.')
}}
[% / %]
                        
{{ cloudflare_video('246a6e1fc8a936c0de617fdf47ff45e8') }}
                        

---
                        
[% .papers %]
{{ paper('“Train one, Classify one, Teach one” - Cross-surgery transfer learning for surgical step recognition',
        'Daniel Neimark, Omri Bar, Maya Zohar, Gregory D. Hager, Dotan Asselmann',
        openreview='https://openreview.net/forum?id=cTB4Qz3RzCl',
        pdf='/proceedings/neimark21.pdf',
        id='C6',
        paper='papers/C6.html',
        proceedings='',
        abstract='Prior work demonstrated the ability of machine learning to automatically recognize surgical workflow steps from videos. However, these studies focused on only a single type of procedure. In this work, we analyze, for the first time, surgical step recognition on four different laparoscopic surgeries: Cholecystectomy, Right Hemicolectomy, Sleeve Gastrectomy, and Appendectomy. Inspired by the traditional apprenticeship model, in which surgical training is based on the Halstedian method, we paraphrase the “see one, do one, teach one” approach for the surgical intelligence domain as “train one, classify one, teach one”. In machine learning, this approach is often referred to as transfer learning. To analyze the impact of transfer learning across different laparoscopic procedures, we explore various time-series architectures and examine their performance on each target domain. We introduce a new architecture, the Time-Series Adaptation Network (TSAN), an architecture optimized for transfer learning of surgical step recognition, and we show how TSAN can be pre-trained using self-supervised learning on a Sequence Sorting task. Such pre-training enables TSAN to learn workflow steps of a new laparoscopic procedure type from only a small number of labeled samples from the target procedure. Our proposed architecture leads to better performance compared to other possible architectures, reaching over 90% accuracy when transferring from laparoscopic Cholecystectomy to the other three procedure types. ')
}}
[% / %]
                        
{{ cloudflare_video('33cb10acca0a612c6d474b2795cb88a9') }}
                        

---
                        
[% .papers %]
{{ paper('Predicting COVID-19 Lung Infiltrate Progression on Chest Radiographs Using Spatio-temporal LSTM based Encoder-Decoder Network',
        'Aishik Konwer, Joseph Bae, Gagandeep Singh, Rishabh Gattu, Syed Ali, Jeremy Green, Tej Phatak, Amit Gupta, Chao Chen, Joel Saltz, Prateek Prasanna',
        openreview='https://openreview.net/forum?id=96BhL_MERil',
        pdf='/proceedings/konwer21.pdf',
        id='C7',
        paper='papers/C7.html',
        proceedings='',
        abstract='Automated analyses of chest imaging in Coronavirus Disease 2019 (COVID-19) have largely focused on a single timepoint, usually at disease presentation, and have not explicitly taken into account temporal disease manifestations. We present a deep learning-based approach for prediction of imaging progression from serial chest radiographs (CXRs) of COVID-19 patients. Our method first utilizes convolutional neural networks (CNNs) for feature extraction from patches within the concerned lung zone, and also from neighboring areas to enhance the contextual phenotypic information. The framework further incorporates two distinct spatio-temporal Long Short Term Memory (LSTM) modules for effective predictions. The first LSTM module captures spatial dependencies between patches and the second exploits the temporal context of sequential CXR scans. The resulting network focuses on critical image regions that provide relevant information for learning the progression of lung infiltrates without the explicit need for infiltrate segmentation. The second LSTM provides an encoded context vector used as an input to a decoder module to predict future severity grades. Our novel multi-institutional dataset comprises sequential CXR scans from N=100 patients. Specifically, our framework predicts zone-wise disease severity for a patient on the last day by learning representations from the previous temporal CXRs. We design two baseline approaches - one using fine-tuned VGG-16 features and the other using radiomic descriptors. Experimental results demonstrate that our proposed approach outperforms both baselines in average accuracy by 10.33% and 12.16%, respectively, in predicting COVID-19 progression severity.')
}}
[% / %]
                        
{{ cloudflare_video('c3fc1ae73541ca0cb63e802885feb952') }}
                        

---
                        
[% .papers %]
{{ paper('Feature-based image registration in structured light endoscopy',
        'Andreas M Kist, Julian Zilker, Michael Döllinger, Marion Semmler',
        openreview='https://openreview.net/forum?id=MzC8X6cMF2r',
        pdf='/proceedings/kist21.pdf',
        id='C8',
        paper='papers/C8.html',
        proceedings='',
        abstract='Images offer a two-dimensional (2D) representation of a three-dimensional (3D) environment. However, in many biomedical tasks, a 3D view is crucial for diagnosis. Projecting structured light, such as a regular laser grid, onto the surface of interest allows to reconstruct its 3D structure. For reconstruction, it is crucial to correctly identify and assign each laser ray to its respective position in the laser grid. Current methods for this task use semi-automatic, yet highly manual annotations. Hence, a fully automatic, reliable method is desired. Here, we show that this assignment can be approached as an image registration. We first separate the laser rays from the background using semantic segmentation. We found that registration of the extracted laser rays directly to the fixed laser grid image fails, when we use state-of-the-art intensity-based image registration techniques, such as ANTs. Using our feature-based custom loss and a deep neural network, we are able to use a U-Net-like architecture to compute deformation fields to successfully register the laser rays onto the fixed image accompanied with a custom post-processing sorting step. Using synthetic data, we show that the network is in general able to learn affine and non-linear transformations. Our method is also robust to missing or occluded rays. Using an ex vivo dataset, we achieved an registration accuracy of 91%. In summary, we provide a new platform to perform feature-based registration and showcase this on a biomedical dataset. In future, we will evaluate different architectural designs and more complex datasets. ')
}}
[% / %]
                        
{{ cloudflare_video('18b3f989af37dbfb3adf7ae46ed72adc') }}
                        

---
                        
[% .papers %]
{{ paper('Intensity Correction and Standardization for Electron Microscopy Data',
        'Oleh Dzyubachyk, Roman I Koning, Aat A Mulder, M. Christina Avramut, Frank GA Faas, Abraham J Koster',
        openreview='https://openreview.net/forum?id=MAUkVcDzDPA',
        pdf='/proceedings/dzyubachyk21.pdf',
        id='C9',
        paper='papers/C9.html',
        proceedings='',
        abstract='Intensity of acquired electron microscopy data is subjected to large variability due to the interplay of many different factors, such as microscope and camera settings used for data acquisition, sample thickness, specimen staining protocol and more. In this work, we developed an efficient method for performing intensity inhomogeneity correction on a single set of combined transmission electron microscopy (TEM) images and demonstrated its positive impact on training a neural network on these data. In addition, we investigated what impact different intensity standardization methods have on the training performance, both for data originating from a single source as well as from several different sources. As a concrete example, we considered the problem of segmenting mitochondria from EM data and demonstrated that we were able to obtain promising results when training our network on a large array of highly-variable in-house TEM data.')
}}
[% / %]
                        
{{ cloudflare_video('bd73a60a985e65dadd32a5e6294367ac') }}
                        