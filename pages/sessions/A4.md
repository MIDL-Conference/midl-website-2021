---
title: "Session A4"
page_class: "program"
---

{% from "_macros.html" import button, paper, video %}

# A4 - Segmentation
##  Wednesday 7th July, 13:45 - 14:30 (UCT+2)


[% .papers %]
{{ paper('Distill DSM: Computationally efficient method for segmentation of medical imaging volumes',
        'Harsh Maheshwari, Vidit Goel, Ramanathan Sethuraman, Debdoot Sheet',
        openreview='https://openreview.net/forum?id=_n48l6YKc6d',
        pdf='/proceedings/maheshwari21.pdf',
        id='A10',
        paper='papers/A10.html',
        proceedings='',
        abstract='Accurate segmentation of volumetric scans like MRI and CT scans is highly demanded for surgery planning in clinical practice, quantitative analysis, and identification of disease. However, accurate segmentation is challenging because of the irregular shape of given organ and large variation in appearances across the slices. In such problems, 3D features are desired in nature which can be extracted using 3D convolutional neural network (CNN). However, 3D CNN is compute and memory intensive to implement due to large number of parameters and can easily over fit, especially in medical imaging where training data is limited. In order to address these problems, we propose a distillation-based depth shift module (Distill DSM). It is designed to enable 2D convolutions to make use of information from neighbouring frames more efficiently. Specifically, in each layer of the network, Distill DSM learns to extract information from a part of the channels and shares it with neighbouring slices, thus facilitating information exchange among neighbouring slices. This approach can be incorporated with any 2D CNN model to enable it to use information across the slices with introducing very few extra learn-able parameters. We have evaluated our model on BRATS 2020, heart, hippocampus, pancreas and prostate dataset. Our model achieves better performance than 3D CNN for heart and prostate datasets and comparable performance on BRATS 2020, pancreas and hippocampus dataset with simply 28\\% of parameters compared to 3D CNN model. ')
}}
[% / %]
                        
{{ video('/videos/full_106_video.mp4') }}
                        
[% .papers %]
{{ paper('Stroke Lesion Outcome Prediction Based on 4D CT Perfusion Data Using Temporal Convolutional Networks',
        'Kimberly Amador, Matthias Wilms, Anthony Winder, Jens Fiehler, Nils Forkert',
        openreview='https://openreview.net/forum?id=0YDEgvfwEW',
        pdf='/proceedings/amador21.pdf',
        id='A11',
        paper='papers/A11.html',
        proceedings='',
        abstract='Acute ischemic stroke is caused by a blockage in the cerebral arteries, resulting in long-term disability and sometimes death. To determine the optimal treatment strategy, a patient-specific assessment is often based on advanced neuroimaging data, such as spatio-temporal (4D) CT Perfusion (CTP) imaging. To date, perfusion maps are typically calculated from 4D CTP data and then thresholded to localize and quantify the stroke lesion core and tissue-at-risk. A few studies have recently developed deep learning methods to predict stroke lesion outcomes from perfusion maps. The basic idea of these is to train a model, using perfusion maps acquired at baseline and their corresponding follow-up images acquired several days after treatment, to automatically estimate the final lesion location and volume in new patients. Nevertheless, model training based on the original 4D CTP scans might be desirable, as they could contain more valuable information not directly represented in perfusion maps. Therefore, we aimed to develop and evaluate a temporal convolutional neural network (TCN) to predict stroke lesion outcomes directly from 4D CTP datasets acquired at admission, without computing any perfusion maps. Using a total of 176 CTP scans, we investigated the impact of the time window size by training the proposed TCN on various numbers of CTP frames: 8, 16, and 32 time points. For comparison purposes, we also trained a convolutional neural network based on perfusion maps. The results show that the model trained on 32 time points yielded significantly higher Dice values (0.33$\\pm$0.21) than the models trained on 8 time points (0.25$\\pm$0.20; P$<$0.05), 16 time points (0.28$\\pm$0.21; P$<$0.001), and perfusion maps (0.23$\\pm$0.18; P$<$0.05). These experiments demonstrate that the proposed model effectively extracts spatio-temporal data from CTP scans to predict stroke lesion outcomes, which leads to better results than using perfusion maps. ')
}}
[% / %]
                        
{{ video('/videos/full_85_video.mp4') }}
                        
[% .papers %]
{{ paper('Direct Inference of Cell Positions using Lens-Free Microscopy and Deep Learning',
        'Philipp Gruening, Falk Nette, Noah Heldt, Ana Cristina Guerra de Souza, Erhardt Barth',
        openreview='https://openreview.net/forum?id=2fpsTsvCgc0',
        pdf='/proceedings/gruening21.pdf',
        id='A12',
        paper='papers/A12.html',
        proceedings='',
        abstract='With in-line holography, it is possible to record biological cells over time in a three-dimensional hydrogel without the need for staining, providing the capability of observing cell behavior in a minimally invasive manner. However, this setup currently requires computationally intensive image-reconstruction algorithms to determine the required cell statistics. In this work, we directly extract cell positions from the holographic data by using deep neural networks and thus avoid several reconstruction steps. We show that our method is capable of substantially decreasing the time needed to extract information from the raw data without loss in quality.')
}}
[% / %]
                        
{{ video('/videos/full_70_video.mp4') }}
                        
[% .papers %]
{{ paper('Common limitations of performance metrics in biomedical image analysis',
        'Annika Reinke, Matthias Eisenmann, Minu Dietlinde Tizabi, Carole H. Sudre, Tim Rädsch, Michela Antonelli, Tal Arbel, Spyridon Bakas, M. Jorge Cardoso, Veronika Cheplygina, Keyvan Farahani, Ben Glocker, Doreen Heckmann-Nötzel, Fabian Isensee, Pierre Jannin, Charles Kahn, Jens Kleesiek, Tahsin Kurc, Michal Kozubek, Bennett A. Landman, Geert Litjens, Klaus Maier-Hein, Anne Lousise Martel, bjoern menze, Henning Müller, Jens Petersen, Mauricio Reyes, Nicola Rieke, Bram Stieltjes, Ronald M. Summers, Sotirios A. Tsaftaris, Bram van Ginneken, Annette Kopp-Schneider, Paul Jäger, Lena Maier-Hein',
        openreview='https://openreview.net/forum?id=76X9Mthzv4X',
        pdf='https://openreview.net/pdf?id=76X9Mthzv4X',
        id='A4',
        paper='papers/A4.html',
        proceedings='',
        abstract="While the importance of automatic biomedical image analysis is increasing at an enormous pace, recent meta-research revealed major flaws with respect to algorithm validation. Performance metrics are key for objective, transparent and comparative performance assessment, but little attention has been given to their pitfalls. Under the umbrella of the Helmholtz Imaging Platform (HIP), three international initiatives - the MICCAI Society\\'s challenge working group, the Biomedical Image Analysis Challenges (BIAS) initiative, as well as the benchmarking working group of the MONAI framework - have now joined forces with the mission to generate best practice recommendations with respect to metrics in medical image analysis. Consensus building is achieved via a Delphi process, a popular tool for integrating opinions in large international consortia. The current document serves as a teaser for the results presentation and focuses on the pitfalls of the most commonly used metric in biomedical image analysis, the Dice Similarity Coefficient (DSC), in the categories of (1) mathematical properties/edge cases, (2) task/metric fit and (3) metric aggregation. Being compiled by a large group of experts from more than 30 institutes worldwide, we believe that our framework could be of general interest to the MIDL community and will improve the quality of biomedical image analysis algorithm validation. ")
}}
[% / %]
                        
{{ video('/videos/short_21_video.mp4') }}
                        
[% .papers %]
{{ paper('VinDr-RibCXR: A Benchmark Dataset for Automatic Segmentation and Labeling of Individual Ribs on Chest X-rays',
        'Hoang Canh Nguyen, Tung Thanh Le, Hieu Pham, Ha Quy Nguyen',
        openreview='https://openreview.net/forum?id=oJi6xpSLdsj',
        pdf='https://openreview.net/pdf?id=oJi6xpSLdsj',
        id='A5',
        paper='papers/A5.html',
        proceedings='',
        abstract='Segmenting and labeling correctly the individual ribs from chest radiograph (CXR) are of significant clinical value for several diagnostic tasks. Developing automatic deep learning (DL) algorithms for this task requires annotated images of the ribs at pixel-level. However, to the best of our knowledge, there exists no such public datasets as well as benchmark protocols for performance evaluation. To solve this problem, we establish a new CXR dataset, namely VinDr-RibCXR, for automatically segmenting and labeling of individual ribs. The VinDr-RibCXR contains 245 posteroanterior CXRs with corresponding segmentation annotations for each rib provided by human experts. Furthermore, we train the state-of-the-art DL-based segmentation models on 196 images from the RibCXR and report performance of those models on an independent test set of 49 images. Our best performing DL model (i.e., Nested U-Net with EfficientNet-B0)  obtains  a  Dice  score  of  0.834 (95% CI, 0.810-0.853). The  sensitivity,  specificity  and  Hausdorff distance are 0.841 (95% CI, 0.812-0.858), 0.998 (95% CI, 0.997-0.998), and 15.453 (95% CI, 13.340-17.450), respectively. These results demonstrate a high-level of performance in labeling of the individual ribs from CXRs. Our study, therefore, serves as a proof of concept and baseline performance for future research. The dataset, codes, and trained DL models will be made publicly available to encourage new advances in this research direction.')
}}
[% / %]
                        
{{ video('/videos/short_36_video.mp4') }}
                        
[% .papers %]
{{ paper('Learning to predict cutting angles from histological human brain sections',
        'Christian Schiffer, Luisa Schuhmacher, Katrin Amunts, Timo Dickscheid',
        openreview='https://openreview.net/forum?id=9CSM4yQmZiN',
        pdf='https://openreview.net/pdf?id=9CSM4yQmZiN',
        id='A6',
        paper='papers/A6.html',
        proceedings='',
        abstract='Studying brain architecture at the cellular level requires histological image analysis of sectioned postmortem samples. We trained a deep neural network to estimate relative angles between the cutting plane and the local 3D brain surface from 2D cortical image patches sampled from microscopic scans of human brain tissue sections. The model allows to automatically identify obliquely cut tissue parts, which often confuse downstream texture classification tasks and typically require specific treatment in image analysis workflows. It has immediate applications for the automated analysis of brain structures, like cytoarchitectonic mapping of the highly convoluted human brain.')
}}
[% / %]
                        
{{ video('/videos/short_6_video.mp4') }}
                        
[% .papers %]
{{ paper('Localizing neurosurgical instruments across domains and in the wild',
        'Markus Philipp, Anna Alperovich, Marielena Gutt-Will, Andrea Mathis, Stefan Saur, Andreas Raabe, Franziska Mathis-Ullrich',
        openreview='https://openreview.net/forum?id=21m0dBCMdd',
        pdf='/proceedings/philipp21.pdf',
        id='A7',
        paper='papers/A7.html',
        proceedings='',
        abstract='Towards computer-assisted neurosurgery, robust methods for instrument localization on neurosurgical microscope video data are needed. Specifically for neurosurgical data, challenges arise from visual conditions such as strong blur and from an unknowingly large variety of instrument types. For neurosurgical domain, instrument localization methods must generalize across different sub-disciplines such as cranial tumor and aneurysm surgeries which exhibit different visual properties. We present and evaluate a methodology towards robust instrument tip localization for neurosurgical microscope data, formulated as coarse saliency prediction. For our analysis, we build a comprehensive dataset comprising in-the-wild data from several neurosurgical sub-disciplines as well as phantom surgeries. Comparing single stream networks using either image or optical flow information, we find complementary performance of both networks. Plain optical flow enables better cross-domain generalization, while the image-based network performs better on surgeries from the training domain. Based on these findings, we present a two-stream architecture that fuses image and optical flow information to utilize the complementary performance of both. Being trained on tumor surgeries, our architecture outperforms both single stream networks and shows improved robustness on data from different neurosurgical sub-disciplines. From our findings, future work must focus more on how to incorporate optical flow information into fusion architectures to further improve cross-domain generalization.')
}}
[% / %]
                        
{{ video('/videos/full_114_video.mp4') }}
                        
[% .papers %]
{{ paper('Weakly Supervised Volumetric Segmentation via Self-taught Shape Denoising Model',
        'Qian He, Shuailin Li, Xuming He',
        openreview='https://openreview.net/forum?id=Koyg3kvH-Mq',
        pdf='/proceedings/he21.pdf',
        id='A8',
        paper='papers/A8.html',
        proceedings='',
        abstract='Weakly supervised segmentation is an important problem in medical image analysis due to the high cost of pixelwise annotation. Prior methods, while often focusing on weak labels of 2D images, exploit few structural cues of volumetric medical images. To address this, we propose a novel weakly-supervised segmentation strategy capable of better capturing 3D shape prior in both model prediction and learning. Our main idea is to extract a self-taught shape representation by leveraging weak labels, and then integrate this representation into segmentation prediction for shape refinement. To this end, we design a deep network consisting of a segmentation module and a shape denoising module, which are trained by an iterative learning strategy. Moreover, we introduce a weak annotation scheme with a hybrid label design for volumetric images, which improves model learning without increasing the overall annotation cost. The empirical experiments show that our approach outperforms existing SOTA strategies on three organ segmentation benchmarks with distinctive shape properties. Notably, we can achieve strong performance with even 10% labeled slices, which is significantly superior to other methods.')
}}
[% / %]
                        
{{ video('/videos/full_69_video.mp4') }}
                        
[% .papers %]
{{ paper('Benefits of Linear Conditioning for Segmentation using Metadata',
        'Andreanne Lemay, Charley Gros, Olivier Vincent, Yaou Liu, Joseph Paul Cohen, Julien Cohen-Adad',
        openreview='https://openreview.net/forum?id=fa176bQAbr',
        pdf='/proceedings/lemay21.pdf',
        id='A9',
        paper='papers/A9.html',
        proceedings='',
        abstract="Medical images are often accompanied by metadata describing the image (vendor, acquisition parameters) and the patient (disease type or severity, demographics, genomics). This metadata is usually disregarded by image segmentation methods. In this work, we adapt a linear conditioning method called FiLM (Feature-wise Linear Modulation) for image segmentation tasks. This FiLM adaptation enables integrating metadata into segmentation models for better performance. We observed an average Dice score increase of 5.1% on spinal cord tumor segmentation when incorporating the tumor type with FiLM. The metadata modulates the segmentation process through low-cost affine transformations applied on feature maps which can be included in any neural network\\'s architecture. Additionally, we assess the relevance of segmentation FiLM layers for tackling common challenges in medical imaging: training with limited or unbalanced number of annotated data, multi-class training with missing segmentations, and model adaptation to multiple tasks. Our results demonstrated the following benefits of FiLM for segmentation: FiLMed U-Net was robust to missing labels and reached higher Dice scores with few labels (up to 16.7%) compared to single-task U-Net. The code is open-source and available at www.ivadomed.org.")
}}
[% / %]
                        
{{ video('/videos/full_80_video.mp4') }}
                        