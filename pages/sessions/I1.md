---
title: "Session I1"
page_class: "program"
sidebar: false
---

{% from "_macros.html" import paper, cloudflare_video %}


# I1 - Interpretability and Explainable AI
##  Friday 9th July, 13:00 - 13:30 (UTC+2)
### Chairs: Ismail Ben Ayed, Arrate Muñoz-Barrutia



---
                        
[% .papers %]
{{ paper('Explainable Image Quality Analysis of Chest X-Rays',
        'Caner Ozer, Ilkay Oksuz',
        openreview='https://openreview.net/forum?id=ln797A8lAb0',
        pdf='/proceedings/ozer21.pdf',
        id='I1',
        paper='papers/I1.html',
        proceedings='',
        abstract='Medical image quality assessment is an important aspect of image acquisition where poor-quality images may lead to misdiagnosis. In addition, manual labelling of image quality after the acquisition is often tedious and can lead to some misleading results. Despite much research on the automated analysis of image quality for tackling this problem, relatively little work has been done for the explanation of the methodologies. In this work, we propose an explainable image quality assessment system and validate our idea on foreign objects in a Chest X-Ray (Object-CXR) dataset. Our explainable pipeline relies on NormGrad, an algorithm, which can efficiently localize the image quality issues with saliency maps of the classifier. We compare our method with a range of saliency detection methods and illustrate the superior performance of NormGrad by obtaining a Pointing Game accuracy of 0.862 on the test dataset of the Object-CXR dataset. We also verify our findings through a qualitative analysis by visualizing attention maps for foreign objects on X-Ray images.')
}}
[% / %]
                        
{{ cloudflare_video('2cdb9a13c304ea0c111b5e1d0c75fec4') }}
                        

---
                        
[% .papers %]
{{ paper('Gifsplanation via Latent Shift: A Simple Autoencoder Approach to Counterfactual Generation for Chest X-rays',
        'Joseph Paul Cohen, Rupert Brooks, Sovann En, Evan Zucker, Anuj Pareek, Matthew P. Lungren, Akshay Chaudhari',
        openreview='https://openreview.net/forum?id=rnunjvgxAMt',
        pdf='/proceedings/cohen21.pdf',
        id='I2',
        paper='papers/I2.html',
        proceedings='',
        abstract='Motivation: Traditional image attribution methods struggle to satisfactorily explain predictions of neural networks. Prediction explanation is important, especially in medical imaging, for avoiding the unintended consequences of deploying AI systems when false positive predictions can impact patient care. Thus, there is a pressing need to develop improved models for model explainability and introspection. Specific problem: A new approach is to transform input images to increase or decrease features which cause the prediction. However, current approaches are difficult to implement as they are monolithic or rely on GANs. These hurdles prevent wide adoption.Our approach: Given an arbitrary classifier, we propose a simple autoencoder and gradient update (Latent Shift) that can transform the latent representation of a specific input image to exaggerate or curtail the features used for prediction. We use this method to study chest X-ray classifiers and evaluate their performance. We conduct a reader study with two radiologists assessing 240 chest X-ray predictions to identify which ones are false positives (half are) using traditional attribution maps or our proposed method.Results: We found low overlap with ground truth pathology masks for models with reasonably high accuracy. However, the results from our reader study indicate that these models are generally looking at the correct features.We also found that the Latent Shift explanation allows a user to have more confidence in true positive predictions compared to traditional approaches (0.15±0.95 in a 5 point scale with p=0.01) with only a small increase in false positive predictions (0.04±1.06 with p=0.57).Accompanying webpage: https://mlmed.org/gifsplanation/Source code: https://github.com/mlmed/gifsplanation')
}}
[% / %]
                        
{{ cloudflare_video('c55f1bcb9f80dd837d87f535f4783c4b') }}
                        

---
                        
[% .papers %]
{{ paper('Understanding and Visualizing Generalization UNets',
        'Abhejit Rajagopal, Vamshi Chowdary Madala, Thomas A Hope, Peder Larson',
        openreview='https://openreview.net/forum?id=V-a5DJCh4Hk',
        pdf='/proceedings/rajagopal21.pdf',
        id='I3',
        paper='papers/I3.html',
        proceedings='',
        abstract="Fully-convolutional neural networks, such as the 2D or 3D UNet, are now pervasive in medical imaging for semantic segmentation, classification, image denoising, domain translation, and reconstruction. However, evaluation of UNet performance, as with most CNNs, has mostly been relegated to evaluation of a few performance metrics (e.g. accuracy, IoU, SSIM, etc.) using the network\\'s final predictions, which provides little insight into important issues such as dataset shift that occur in clinical application. In this paper, we propose techniques for understanding and visualizing the generalization performance of UNets in image classification and regression tasks, giving rise to metrics that are indicative of performance on a withheld test-set without the need for groundtruth annotations.")
}}
[% / %]
                        
{{ cloudflare_video('b0d63f8886cc83c6640e75fdde1e138c') }}
                        