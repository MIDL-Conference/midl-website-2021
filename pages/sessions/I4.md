---
title: "Session I4"
page_class: "program"
---

{% from "_macros.html" import button, paper, video %}

# I4 - Transfer Learning and Domain Adaptation
##  Friday 9th July, 13:45 - 14:30 (UCT+2)


[% .papers %]
{{ paper('Partial transfusion: on the expressive influence of trainable batch norm parameters for transfer learning',
        'Fahdi Kanavati, Masayuki Tsuneki',
        openreview='https://openreview.net/forum?id=TjwDWRdfZpg',
        pdf='/proceedings/kanavati21.pdf',
        id='I10',
        paper='papers/I10.html',
        proceedings='',
        abstract='Transfer learning from ImageNet is the go-to approach when applying deep learning to medical images. The approach is either to fine-tune a pre-trained model or use it as a feature extractor.Most modern architecture contain batch normalisation layers, and fine-tuning a model with such layers requires taking a few precautions as they consist of trainable and non-trainable weights and have two operating modes: training and inference. Attention is primarily given to the non-trainable weights used during inference, as they are the primary source of unexpected behaviour or degradation in performance during transfer learning. It is typically recommended to fine-tune the model with the batch normalisation layers kept in inference mode during both training and inference. In this paper, we pay closer attention instead to the trainable weights of the batch normalisation layers, and we explore their expressive influence in the context of transfer learning.We find that only fine-tuning the trainable weights (scale and centre) of the batch normalisation layers leads to similar performance as to fine-tuning all of the weights, with the added benefit of faster convergence. We demonstrate this on a variety of seven publicly available medical imaging datasets, using four different model architectures. ')
}}
[% / %]
                        
{{ video('/videos/full_41_video.mp4') }}
                        
[% .papers %]
{{ paper('CNN and Deep Sets for End-to-End Whole Slide Image Representation Learning',
        'Sobhan Hemati, Shivam Kalra, Cameron Meaney, Morteza Babaie, Ali Ghodsi, Hamid Tizhoosh',
        openreview='https://openreview.net/forum?id=BX0kKB1zB1Q',
        pdf='/proceedings/hemati21.pdf',
        id='I11',
        paper='papers/I11.html',
        proceedings='',
        abstract='Digital pathology has enabled us to capture, store and analyze scanned biopsy samples as digital images. Recent advances in deep learning are contributing to computational pathology to improve diagnosis and treatment. However, considering challenges inherent to whole slide images (WSIs), it is not easy to employ deep learning in digital pathology. More importantly, computational bottlenecks induced by the gigapixel WSIs make it difficult to use deep learning for end-to-end image representation. To mitigate this challenge, many patch-based approaches have been proposed. Although patching WSIs enables us to use deep learning, we end up with a bag of patches or set representation which makes downstream tasks non-trivial. More importantly, considering set representation per WSI, it is not clear how one can obtain similarity between two WSIs (sets) for tasks like image search matching. To address this challenge, we propose a neural network based on Convolutions Neural Network (CNN) and Deep Sets to learn one permutation invariant vector representation per WSI in an end-to-end manner. Considering available labels at the WSI level - namely, primary site and cancer subtypes - we train the proposed network in a multi-label setting to encode both primary site and diagnosis. Having in mind that every primary site has its own specific cancer subtypes, we propose to use the predicted label for the primary site to recognize the cancer subtype. The proposed architecture is used for transfer learning of WSIs and validated two different tasks, i.e., search and classification. The results show that the proposed architecture can be used to obtain WSI representations that achieve better performance both in terms of retrieval performance and search time against \\emph{Yottixel}, a recently developed search engine for pathology images. Further, the model achieved competitive performance against the state-of-art in lung cancer classification.')
}}
[% / %]
                        
{{ video('/videos/full_90_video.mp4') }}
                        
[% .papers %]
{{ paper('Tailoring automated data augmentation to H&E-stained histopathology',
        'Khrystyna Faryna, Jeroen van der Laak, Geert Litjens',
        openreview='https://openreview.net/forum?id=JrBfXaoxbA2',
        pdf='/proceedings/faryna21.pdf',
        id='I12',
        paper='papers/I12.html',
        proceedings='',
        abstract='Convolutional neural networks (CNN) are sensitive to domain shifts, which can result in poor generalization. In medical imaging, data acquisition conditions differ among institutions, which leads to variations in image properties and thus domain shift. Stain variation in histopathological slides is a prominent example. Data augmentation is one way to make CNNs robust to varying forms of domain shift, but requires extensive hyperparameter tuning. Due to the large search space, this is cumbersome and often leads to sub-optimal generalization performance. In this work, we focus on automated and computationally efficient data augmentation policy selection for histopathological slides. Building upon the RandAugment framework, we introduce several domain-specific modifications relevant to histopathological images, increasing generalizability. We test these modifications on H\\&E-stained histopathology slides from Camelyon17 dataset. Our proposed framework outperforms the state-of-the-art manually engineered data augmentation strategy, achieving an area under the ROC curve of 0.964 compared to 0.958, respectively.')
}}
[% / %]
                        
{{ video('/videos/full_72_video.mp4') }}
                        
[% .papers %]
{{ paper('Scopeformer: n-CNN-ViT hybrid model for Intracranial hemorrhage subtypes classification',
        'Yassine Barhoumi, Ghulam Rasool',
        openreview='https://openreview.net/forum?id=M1VznPOV5jV',
        pdf='https://openreview.net/pdf?id=M1VznPOV5jV',
        id='I4',
        paper='papers/I4.html',
        proceedings='',
        abstract='We propose a feature generator backbone composed of an ensemble of convolutional neural networks (CNNs) to improve the recently emerging Vision Transformer (ViT) models. We tackled the RSNA intracranial hemorrhage classification problem, i.e., identifying various hemorrhage  types  from  computed  tomography  (CT)  slices.   We  show  that  by gradually stacking  several  feature  maps  extracted  using  multiple  Xception  CNNs,  we  can develop a  feature-rich  input  for  the  ViT  model.   Our  approach  allowed  the  ViT  model  to  pay attention to relevant features at multiple levels.  Moreover, pretraining the ”n” CNNs using various paradigms leads to a diverse feature set and further improves the performance of the proposed n-CNN-ViT. We achieved a test accuracy of 98.04% with a weighted logarithmic loss value of 0.0708.  The proposed architecture is modular and scalable in both the number of CNNs used for feature extraction and the size of the ViT.')
}}
[% / %]
                        
{{ video('/videos/short_106_video.mp4') }}
                        
[% .papers %]
{{ paper('Robust medical image segmentation by adapting neural networks for each test image',
        'Neerav Karani, Ertunc Erdil, Krishna Chaitanya, Ender Konukoglu',
        openreview='https://openreview.net/forum?id=tv_pkmFzdC',
        pdf='https://openreview.net/pdf?id=tv_pkmFzdC',
        id='I5',
        paper='papers/I5.html',
        proceedings='',
        abstract='Performance of convolutional neural networks (CNNs) used for medical image analyses degrades markedly when training and test images differ in terms of their acquisition details, such as the scanner model or the protocol. We tackle this issue for the task of image segmentation by adapting a CNN ($C$) for each test image. Specifically, we design $C$ as a concatenation of a shallow normalization CNN ($N$), followed by a deep CNN ($S$) that segments the normalized image. At test time, we adapt $N$ for each test image, guided by an implicit prior on the predicted labels, which is modelled using an independently trained denoising autoencoder ($D$). The method is validated on multi-center MRI datasets of 3 anatomies. This article is a short version of the journal paper~\\cite{karani2021test}.')
}}
[% / %]
                        
{{ video('/videos/short_76_video.mp4') }}
                        
[% .papers %]
{{ paper('Quantifying the Scanner-Induced Domain Gap in Mitosis Detection',
        'Marc Aubreville',
        openreview='https://openreview.net/forum?id=OcATYbGIxv4',
        pdf='https://openreview.net/pdf?id=OcATYbGIxv4',
        id='I6',
        paper='papers/I6.html',
        proceedings='',
        abstract='Automated detection of mitotic figures in histopathology images has seen vast improvements, thanks to modern deep learning-based pipelines. Application of these methods, however, is in practice limited by strong variability of images between labs. This results in a domain shift of the images, which causes a performance drop of the models. Hypothesizing that the scanner device plays a decisive role in this effect, we evaluated the susceptibility of a standard mitosis detection approach to the domain shift introduced by using a different whole slide scanner. Our work is based on the MICCAI-MIDOG challenge 2021 data set, which includes 200 tumor cases of human breast cancer and four scanners.  Our work indicates that the domain shift induced not by biochemical variability but purely by the choice of acquisition device is underestimated so far. Models trained on images of the same scanner yielded an average F1 score of 0.683, while models trained on a single other scanner only yielded an average F1 score of 0.325. Training on another multi-domain mitosis dataset led to mean F1 scores of 0.52. We found this not to be reflected by domain-shifts measured as proxy A distance-derived metric.')
}}
[% / %]
                        
{{ video('/videos/short_13_video.mp4') }}
                        
[% .papers %]
{{ paper('Echocardiographic Phase Detection Using Neural Networks',
        'Elisabeth Sarah Lane, Neda Azarmehr, Jevgeni Jevsikov, James P Howard, Matthew Shun-shin, Darrel P Francis, Massoud Zolgharni',
        openreview='https://openreview.net/forum?id=uEuoKy2hUkm',
        pdf='https://openreview.net/pdf?id=uEuoKy2hUkm',
        id='I7',
        paper='papers/I7.html',
        proceedings='',
        abstract='Accurate identification of end-diastolic and end-systolic frames in echocardiographic cine loops is essential when measuring cardiac function. Manual selection by human experts is challenging and error prone. This paper presents a deep neural network trained and tested on multi-centre patient data for accurate phase detection in apical four-chamber videos of arbitrary length, spanning several heartbeats, with performance indistinguishable from that of human experts.')
}}
[% / %]
                        
{{ video('/videos/short_1_video.mp4') }}
                        
[% .papers %]
{{ paper('HAD-Net: A Hierarchical Adversarial Knowledge Distillation Network for Improved Enhanced Tumour Segmentation Without Post-Contrast Images',
        'Saverio Vadacchino, Raghav Mehta, Nazanin Mohammadi Sepahvand, Brennan Nichyporuk, James J. Clark, Tal Arbel',
        openreview='https://openreview.net/forum?id=48UgSFrNR2',
        pdf='/proceedings/vadacchino21.pdf',
        id='I8',
        paper='papers/I8.html',
        proceedings='',
        abstract='Segmentation of enhancing tumours or lesions from MRI is important for detecting new disease activity in many clinical contexts. However, accurate segmentation requires the inclusion of medical images (e.g., T1 post-contrast MRI) acquired after injecting patients with a contrast agent (e.g., Gadolinium), a process no longer thought to be safe. Although a number of modality-agnostic segmentation networks have been developed over the past few years, they have been met with limited success in the context of enhancing pathology segmentation. In this work, we present HAD-Net, a novel offline adversarial knowledge distillation (KD) technique, whereby a pre-trained teacher segmentation network, with access to all MRI sequences, teaches a student network, via hierarchical adversarial training, to better overcome the large domain shift presented when crucial images are absent during inference. In particular, we apply HAD-Net to the challenging task of enhancing tumour segmentation when access to post-contrast imaging is not available. The proposed network is trained and tested on the BraTS 2019 brain tumour segmentation challenge dataset, where it achieves performance improvements in the ranges of 16% - 26% over (a) recent modality-agnostic segmentation methods (U-HeMIS, U-HVED), (b) KD-Net adapted to this problem, (c) the pre-trained student network and (d) a non-hierarchical version of the network (AD-Net), in terms of Dice scores for enhancing tumour (ET). The network also shows improvements in tumour core (TC) Dice scores. Finally, the network outperforms both the baseline student network and AD-Net in terms of uncertainty quantification for enhancing tumour segmentation based on the BraTS 2019 uncertainty challenge metrics. Our code is publicly available at: https://github.com/SaverioVad/HAD_Net')
}}
[% / %]
                        
{{ video('/videos/full_88_video.mp4') }}
                        
[% .papers %]
{{ paper('An MRF-UNet Product of Experts for Image Segmentation',
        'Mikael Brudfors, Yaël Balbastre, John Ashburner, Geraint Rees, Parashkev Nachev, Sebastien Ourselin, M. Jorge Cardoso',
        openreview='https://openreview.net/forum?id=PoIV8EXQDjn',
        pdf='/proceedings/brudfors21.pdf',
        id='I9',
        paper='papers/I9.html',
        proceedings='',
        abstract='While convolutional neural networks (CNNs) trained by back-propagation have seen unprecedented success at semantic  segmentation tasks, they are known to struggle on out-of-distribution data. Markov random fields (MRFs) on the other hand, encode simpler distributions over labels that, although less flexible than UNets, are less prone to over-fitting. In this paper, we propose to fuse both strategies by computing the product of distributions of a UNet and an MRF. As this product is intractable, we solve for an approximate distribution using an iterative mean-field approach. The resulting MRF-UNet is trained jointly by back-propagation. Compared to other works using conditional random fields (CRFs), the MRF has no dependency on the imaging data, which should allow for less over-fitting. We show on 3D neuroimaging data that this novel network improves generalisation to out-of-distribution samples. Furthermore, it allows the overall number of parameters to be reduced while preserving high accuracy. These results suggest that a classic MRF smoothness prior can allow for less over-fitting when principally integrated into a CNN model. Our implementation is available at https://github.com/balbasty/nitorch.')
}}
[% / %]
                        
{{ video('/videos/full_107_video.mp4') }}
                        