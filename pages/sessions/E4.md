---
title: "Session E4"
page_class: "program"
---

{% from "_macros.html" import paper, cloudflare_video %}


# E4 - Image Registration / Synthesis
##  Thursday 8th July, 13:45 - 14:30 (UCT+2)
### Chairs: Alessa Hering, Herv√© Lombaert



---
                        
[% .papers %]
{{ paper('Cycle Consistent Embedding of 3D Brains with Auto-Encoding Generative Adversarial Networks',
        'Shibo Xing, Harsh Sinha, Seong Jae Hwang',
        openreview='https://openreview.net/forum?id=jgBzGIG-kB',
        pdf='https://openreview.net/pdf?id=jgBzGIG-kB',
        id='E10',
        paper='papers/E10.html',
        proceedings='',
        abstract='Modern generative adversarial networks (GANs) have been enabling the realistic generation of full 3D brain images by sampling from a latent space prior Z (i.e., random vectors) and mapping it to realistic images in X (e.g., 3D MRIs). To address the ubiquitous mode collapse issue, recent works have strongly imposed certain characteristics such as Gaussianness to the prior by also explicitly mapping X to Z via encoder. These efforts, however, fail to accurately map 3D brain images to the desirable prior, which the generator assumes to be sampling the random vectors from. On the other hand, Variational Auto-Encoding GAN (VAE-GAN) solves mode collapse by enforcing Gaussianness by two learned parameter, yet causes blurriness in images. In this work, we show how our cycle consistent embedding} GAN (CCE-GAN) both accurately encodes 3D MRIs to the standard normal prior, and maintains the quality of the generated images. We achieve this without a network-based code discriminator via the Wasserstein measure. We quantitatively and qualitatively assess the embeddings and the generated 3D MRIs using healthy T1-weighted MRIs from ADNI.')
}}
[% / %]
                        
{{ cloudflare_video('fe998757d99d3f9914cf5614db0b019d') }}
                        

---
                        
[% .papers %]
{{ paper('Efficient and Accurate Spatial-Temporal Denoising Network for Low-dose CT Scans',
        'Leihao Wei, William Hsu',
        openreview='https://openreview.net/forum?id=XHWqF4DlRr0',
        pdf='https://openreview.net/pdf?id=XHWqF4DlRr0',
        id='E11',
        paper='papers/E11.html',
        proceedings='',
        abstract='While deep-learning-based imaging denoising techniques can improve the quality of low-dose computed tomography (CT) scans, repetitive 3D convolution operations cost significant computation resources and time. We present an efficient and accurate spatial-temporal convolution method to accelerate an existing denoising network based on the SRResNet. We trained and evaluated our model on our dataset containing 184 low-dose chest CT scans. We compared the performance of the proposed spatial-temporal convolution network to the SRResNet with full 3D convolutional layers. Using 8-bit quantization, we demonstrated a 7-fold speed-up during inference. Using lung nodule characterization as a driving task, we analyzed the impact on image quality and radiomic features. Our results show that our method achieves better perceptual quality, and the outputs are consistent with the SRResNet baseline outputs for some radiomics features (31 out of 57 total features). These observations together demonstrate that the proposed spatial-temporal method can be potentially useful for clinical applications where the computational resource is limited. ')
}}
[% / %]
                        
{{ cloudflare_video('c70c5f00d07212d92d5f969072da6ff0') }}
                        

---
                        
[% .papers %]
{{ paper('Synthesis of Diabetic Retina Fundus Images Using Semantic Label Generation',
        'Joon-Ho Son, Amir Alansary, Daniel Rueckert, Bernhard Kainz, Benjamin Hou',
        openreview='https://openreview.net/forum?id=wiKDehhdnz',
        pdf='https://openreview.net/pdf?id=wiKDehhdnz',
        id='E12',
        paper='papers/E12.html',
        proceedings='',
        abstract='Automatic segmentation of retina lesions have been a long standing and challenging task for learning based models, mostly due to the lack of available and accurate lesion segmentation datasets. In this paper, we propose a two-step process for generating photo-realistic fundus images conditioned on synthetic \\"ground truth\\" semantic labels, and demonstrate its potential for further downstream tasks, such as, but not limited to; automated grading of diabetic retinopathy, dataset balancing, creating image examples for trainee ophthalmologists, etc.')
}}
[% / %]
                        
{{ cloudflare_video('b85756ed0dc46c3bf03b2d688768ab1b') }}
                        

---
                        
[% .papers %]
{{ paper('Rethinking the Design of Learning based Inter-Patient Registration using Deformable Supervoxels ',
        'Mattias P Heinrich',
        openreview='https://openreview.net/forum?id=zZA5TpNdC4Z',
        pdf='https://openreview.net/pdf?id=zZA5TpNdC4Z',
        id='E4',
        paper='papers/E4.html',
        proceedings='',
        abstract='Deep learning has the potential to substantially improve inter-subject alignment for shape and atlas analysis. So far most highly accurate supervised approaches require dense manual annotations and complex multi-level architectures but may still be susceptible to label bias. We present a radically different approach for learning to estimate large deformations without expert supervision. Instead of regressing displacements, we train a 3D DeepLab network to predict automatic supervoxel segmentations. To enable consistent supervoxel labels, we use the warping field of a conventional approach and increase the accuracy by sampling multiple complementary over-segmentations. We experimentally demonstrate that 1) our deformable supervoxels are less sensitive to large initial misalignment and can combine linear and nonlinear registration and 2) using this self-supervised classification loss is more robust to noisy ground truth and leads to better convergence than direct regression as supervision.')
}}
[% / %]
                        
{{ cloudflare_video('36c70776fc40277790fd3297bc260106') }}
                        

---
                        
[% .papers %]
{{ paper('Semi-supervised Image-to-Image translation for robust image registration',
        'henrik skibbe, akiya watakabe, Febrian Rachmadi, Carlos Enrique Gutierrez, Ken Nakae, tetsuo yamamori',
        openreview='https://openreview.net/forum?id=GOhAojdaLg',
        pdf='https://openreview.net/pdf?id=GOhAojdaLg',
        id='E5',
        paper='papers/E5.html',
        proceedings='',
        abstract='The Japan Brain/MINDS Project aims at studying the neural networks controlling higher brain functions in the marmoset. As part of it, we develop an image processing pipeline for marmoset brain imaging data, where various microscopy images of different modalities need to be co-registered. In initial experiments, multi-modal image registration frequently failed due to an erroneous initialization. Our data set includes images of Nissl stained brain sections, backlit images as well as images of neural tracer injections using two-photon microscopy. More than 10000 high-resolution 2D images required co-registration, a large amount that demands a reliable automation process. We implemented a semi-supervised image-to-image translation which allowed a robust image alignment initialization. With such an initial alignment, all images can be successfully registered using a state-of-the-art multi-modal image registration algorithm.')
}}
[% / %]
                        
{{ cloudflare_video('12456ac038e423cbdd9a51867ea7103b') }}
                        

---
                        
[% .papers %]
{{ paper('ViT-V-Net: Vision Transformer for Unsupervised Volumetric Medical Image Registration',
        'Junyu Chen, Yufan He, Eric Frey, Ye Li, Yong Du',
        openreview='https://openreview.net/forum?id=h3HC1EU7AEz',
        pdf='https://openreview.net/pdf?id=h3HC1EU7AEz',
        id='E6',
        paper='papers/E6.html',
        proceedings='',
        abstract='In the last decade, convolutional neural networks (ConvNets) have dominated and achieved state-of-the-art performances in a variety of medical imaging applications. However, the performances of ConvNets are still limited by lacking the understanding of long-range spatial relations in an image. The recently proposed Vision Transformer (ViT) for image classification uses a purely self-attention-based model that learns long-range spatial relations to focus on the relevant parts of an image. Nevertheless, ViT emphasizes the low-resolution features because of the consecutive downsamplings, result in a lack of detailed localization information, making it unsuitable for image registration. Recently, several ViT-based image segmentation methods have been combined with ConvNets to improve the recovery of detailed localization information. Inspired by them, we present ViT-V-Net, which bridges ViT and ConvNet to provide volumetric medical image registration. The experimental results presented here demonstrate that the proposed architecture achieves superior performance to several top-performing registration methods.')
}}
[% / %]
                        
{{ cloudflare_video('1dee9a737a9d785259de7c0f7fe7e78f') }}
                        

---
                        
[% .papers %]
{{ paper('Learning a Metric without Supervision: Multimodal Registration using Synthetic Cycle Discrepancy',
        'Hanna Siebert, Lasse Hansen, Mattias P Heinrich',
        openreview='https://openreview.net/forum?id=sua3vlnkmEv',
        pdf='https://openreview.net/pdf?id=sua3vlnkmEv',
        id='E7',
        paper='papers/E7.html',
        proceedings='',
        abstract='Training deep learning based medical image registration methods involves the challenge of finding a suitable metric. To avoid the difficulty of choosing a metric for multimodal image registration, we propose a completely new concept relying on geometric instead of metric supervision with three-way registration cycles. Therefore, we create a synthetic image by applying a synthetic transformation on one of the input images. This leads to cycles that for each pair of input images comprise two multimodal transformations to be estimated and one known synthetic monomodal transformation. We minimise the discrepancy between the combined multimodal transformations and the synthetic monomodal transformation. By minimising this cycle discrepancy, we are able to learn multimodal registration between CT and MRI without metric supervision. Our method outperforms state-of-the-art metric supervision and comes very close to fully-supervised learning with ground truth labels. ')
}}
[% / %]
                        
{{ cloudflare_video('bfd9ba4d5873e60273a645350bc8154e') }}
                        

---
                        
[% .papers %]
{{ paper('Learning Diffeomorphic and Modality-invariant Registration using B-splines',
        'Huaqi Qiu, Chen Qin, Andreas Schuh, Kerstin Hammernik, Daniel Rueckert',
        openreview='https://openreview.net/forum?id=eSI9Qh2DJhN',
        pdf='/proceedings/qiu21.pdf',
        id='E8',
        paper='papers/E8.html',
        proceedings='',
        abstract='We present a deep learning (DL) registration framework for fast mono-modal and multi-modal image registration using differentiable mutual information and diffeomorphic B-spline free-form deformation (FFD). Deep learning registration has been shown to achieve competitive accuracy and significant speedups from traditional iterative registration methods. In this paper, we propose to use a B-spline FFD parameterisation of Stationary Velocity Field (SVF) to in DL registration in order to achieve smooth diffeomorphic deformation while being computationally-efficient. In contrast to most DL registration methods which use intensity similarity metrics that assume linear intensity relationship, we apply a differentiable variant of a classic similarity metric, mutual information, to achieve robust mono-modal and multi-modal registration. We carefully evaluated our proposed framework on mono- and multi-modal registration using 3D brain MR images and 2D cardiac MR images.')
}}
[% / %]
                        
{{ cloudflare_video('b2338cd525b89da77466dd1b12a5a5d1') }}
                        

---
                        
[% .papers %]
{{ paper('A hybrid model- and deep learning-based framework for functional lung image synthesis from non-contrast multi-inflation CT',
        'Joshua Russell Astley, Alberto M Biancardi, Helen Marshall, Guilhem J Collier, Paul JC Hughes, Jim M Wild, Bilal A Tahir',
        openreview='https://openreview.net/forum?id=_BIpUlEB6ff',
        pdf='https://openreview.net/pdf?id=_BIpUlEB6ff',
        id='E9',
        paper='papers/E9.html',
        proceedings='',
        abstract='Hyperpolarized gas MRI can visualize and quantify regional lung ventilation with exquisite detail but requires highly specialized equipment and exogenous contrast. Alternative, non-contrast techniques, including CT-based models of ventilation have shown moderate spatial correlations with hyperpolarized gas MRI. Here, we propose a hybrid framework that integrates CT-ventilation modelling and deep learning approaches. The hybrid model/DL framework generated synthetic ventilation images which accurately replicated gross ventilation defects in hyperpolarized gas MRI scans, significantly outperforming other model- and DL-only approaches. Our results show that a synergy between conventional CT-ventilation modelling and DL can improve the performance of functional lung image synthesis.')
}}
[% / %]
                        
{{ cloudflare_video('82e91474c8d0ebfb230720ab16042e75') }}
                        