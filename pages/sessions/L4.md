---
title: "Session L4"
page_class: "program"
---

{% from "_macros.html" import button, paper, video %}

# L4 - Detection and Diagnosis 2
##  Friday 9th July, 16:45 - 17:30 (UCT+2)


[% .papers %]
{{ paper('Prediction of Ki67 scores from H&E stained breast cancer sections using convolutional neural networks',
        'Philippe Weitz, Balazs Acs, Johan Hartman, Mattias Rantalainen',
        openreview='https://openreview.net/forum?id=W9sz0zHk33h',
        pdf='https://openreview.net/pdf?id=W9sz0zHk33h',
        id='L4',
        paper='papers/L4.html',
        proceedings='',
        abstract='Ki67 is an established marker of proliferation in breast cancer, but currently has limited clinical value due to limitations of the analytical validity of immunohistochemistry (IHC) -based Ki67 scoring. While the inter-assessor variability of scoring can be improved through image analysis software, Ki67 IHC also suffers from a lack of standardized staining protocols and is not part of routine pathology workflow in most countries. This could potentially be alleviated through directly predicting Ki67 scores from routine hematoxylin and eosin (H\\&E) stained whole-slide-images (WSIs). We compared four different deep learning based approaches to  predict Ki67 scores from routine H\\&E stained WSIs in a dataset that consists of matched H\\&E and Ki67 WSIs from 126 breast cancer patients, resulting in a Spearman correlation between WSI cancer ROI averages of 0.546 for the best performing model in a 5-fold cross-validation (CV). These findings suggest that it is possible to predict the Ki67 score from H\\&E stained WSIs, but validation in a larger cohort is required to meaningfully distinguish the performance of the methods that were investigated. ')
}}
[% / %]
                        
{{ video('/videos/short_80_video.mp4') }}
                        
[% .papers %]
{{ paper('Feedback Graph Attention Convolutional Network for MR Images Enhancement by Exploring Self-Similarity Features',
        'Xiaobin Hu, Yanyang Yan, Wenqi Ren, Hongwei Li, Amirhossein Bayat, yu zhao, bjoern menze',
        openreview='https://openreview.net/forum?id=k1BSWQqHoMV',
        pdf='/proceedings/hu21.pdf',
        id='L5',
        paper='papers/L5.html',
        proceedings='',
        abstract='Artifacts, blur, and noise are the common distortions degrading MRI images during the acquisition process, and deep neural networks have been demonstrated to help in improving image quality. To well exploit global structural information and self-similarity details, we propose a novel MR image enhancement network, named Feedback Graph Attention Convolutional Network (FB-GACN).As a key innovation, we consider the global structure of an image by building a graph network from image sub-regions that we consider to be node features, linking them non-locally according to their similarity. The proposed model consists of three main parts:1) The parallel graph similarity branch and content branch, where the graph similarity branch aims at exploiting the similarity and symmetry across different image sub-regions in low-resolution feature space and provides additional priors for the content branch to enhance texture details.2) A feedback mechanism with a recurrent structure to refine low-level representations with high-level information and generate powerful high-level texture details by handling the feedback connections. 3) A reconstruction to remove the artifacts and recover super-resolution images by using the estimated sub-region self-similarity priors obtained from the graph similarity branch. We evaluate our method on two image enhancement tasks: i) cross-protocol super resolution of diffusion MRI; ii) artifact removal of FLAIR MR images. Experimental results demonstrate that the proposed algorithm outperforms the state-of-the-art methods.')
}}
[% / %]
                        
{{ video('/videos/full_10_video.mp4') }}
                        
[% .papers %]
{{ paper('Unifying Brain Age Prediction and Age-Conditioned Template Generation with a Deterministic Autoencoder',
        'Pauline Mouches, Matthias Wilms, Deepthi Rajashekar, Sonke Langner, Nils Forkert',
        openreview='https://openreview.net/forum?id=9ClUQ2ELJap',
        pdf='/proceedings/mouches21.pdf',
        id='L6',
        paper='papers/L6.html',
        proceedings='',
        abstract='Age-related morphological brain changes are known to be different in healthy and disease-affected aging. Biological brain age estimation from MRI scans is a common way to quantify this effect whereas differences between biological and chronological age indicate degenerative processes. The ability to visualize and analyze the morphological age-related changes in the image space directly is essential to improve the understanding of brain aging. In this work, we propose a novel deep learning based approach to unify biological brain age estimation and age-conditioned template creation in a single, consistent model. We achieve this by developing a deterministic autoencoder that successfully disentangles age-related morphological changes and subject-specific variations. This allows its use as a brain age regressor as well as a generative brain aging model. The proposed approach demonstrates accurate biological brain age prediction, and realistic generation of age-conditioned brain templates and simulated age-specific brain images when applied to a database of more than 2000 subjects.')
}}
[% / %]
                        
{{ video('/videos/full_145_video.mp4') }}
                        
[% .papers %]
{{ paper('GOAL: Gist-set Online Active Learning for Efficient Chest X-ray Image Annotation',
        'Chanh Nguyen, Minh Thanh Huynh, Minh Quan Tran, Ngoc Hoang Nguyen, Mudit Jain, Van Doan Ngo, Tan Duc Vo, Trung Bui, Steven Quoc Hung Truong',
        openreview='https://openreview.net/forum?id=boTEEpM8mu',
        pdf='/proceedings/nguyen21.pdf',
        id='L7',
        paper='papers/L7.html',
        proceedings='',
        abstract='Deep learning in medical image analysis often requires an extensive amount of high-quality labeled data for training to achieve Human-level accuracy. We propose Gist-set Online Active Learning (GOAL), a novel solution for limited high-quality labeled data in medical imaging analysis. Our approach advances the existing active learning methods in three aspects. Firstly, we improve the classification performance with fewer manual annotations by presenting a sample selection strategy called gist set selection. Secondly, unlike traditional methods focusing only on random uncertain samples of low prediction confidence, we propose a new method in which only informative uncertain samples are selected for human annotation. Thirdly, we propose an application of online learning where high-confidence samples are automatically selected, iteratively assigned, and pseudo-labels are updated. We validated our approach on two private and one public dataset. The experimental results show that, by applying GOAL, we can reduce required labeled data up to 88% while maintaining the same F1 scores compared to the models trained on full datasets')
}}
[% / %]
                        
{{ video('/videos/full_92_video.mp4') }}
                        
[% .papers %]
{{ paper('EPIC-Survival: End-to-end Part Inferred Clustering for Survival Analysis, with Prognostic Stratification Boosting',
        'Hassan Muhammad, Chensu Xie, Carlie S Sigel, Michael Doukas, Lindsay Alpert, Amber Lea Simpson, Thomas J Fuchs',
        openreview='https://openreview.net/forum?id=JSSwHS_GU63',
        pdf='/proceedings/muhammad21.pdf',
        id='L8',
        paper='papers/L8.html',
        proceedings='',
        abstract='Histopathology-based survival modelling has two major hurdles. Firstly, a well-performing survival model has minimal clinical application if it does not contribute to the stratification of a cancer patient cohort into different risk groups, preferably driven by histologic morphologies. In the clinical setting, individuals are not given specific prognostic predictions, but are rather predicted to lie within a risk group which has a general survival trend. Thus, It is imperative that a survival model produces well-stratified risk groups. Secondly, until now, survival modelling was done in a two-stage approach (encoding and aggregation). EPIC-Survival bridges encoding and aggregation into an end-to-end survival modelling approach, while introducing stratification boosting to encourage the model to not only optimize ranking, but also to discriminate between risk groups. In this study we show that EPIC-Survival performs better than other approaches in modelling intrahepatic cholangiocarcinoma (ICC), a historically difficult cancer to model. We found that stratification boosting further improves model performance and helps identify specific histologic differences, not commonly sought out in ICC.')
}}
[% / %]
                        
{{ video('/videos/full_49_video.mp4') }}
                        
[% .papers %]
{{ paper('CheXseg: Combining Expert Annotations with DNN-generated Saliency Maps for X-ray Segmentation',
        'Soham Uday Gadgil, Mark Endo, Emily Wen, Andrew Y. Ng, Pranav Rajpurkar',
        openreview='https://openreview.net/forum?id=eA7PGMYmHFA',
        pdf='/proceedings/gadgil21.pdf',
        id='L9',
        paper='papers/L9.html',
        proceedings='',
        abstract='Medical image segmentation models are typically supervised by expert annotations at the pixel-level, which can be expensive to acquire. In this work, we propose a method that combines the high quality of pixel-level expert annotations with the scale of coarse DNN-generated saliency maps for training multi-label semantic segmentation models. We demonstrate the application of our semi-supervised method, which we call CheXseg, on multi-label chest X-ray interpretation. We find that CheXseg improves upon the performance (mIoU) of fully-supervised methods that use only pixel-level expert annotations by 9.7% and weakly-supervised methods that use only DNN-generated saliency maps by 73.1%. Our best method is able to match radiologist agreement on three out of ten pathologies and reduces the overall performance gap by 57.2% as compared to weakly-supervised methods.')
}}
[% / %]
                        
{{ video('/videos/full_31_video.mp4') }}
                        